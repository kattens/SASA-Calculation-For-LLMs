{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/SASA-Calculation-For-LLMs/blob/main/Dummy_Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e39a80b-6dfc-41bd-926c-12f22da749cf",
      "metadata": {
        "id": "7e39a80b-6dfc-41bd-926c-12f22da749cf"
      },
      "source": [
        "# Code Agenda:\n",
        "- Provide global sequence and SASA values as separate inputs.\n",
        "- Use the local sequence as the label.\n",
        "- No new tokens are added; it's a simple prediction task.\n",
        "- Build up from this.\n",
        "\n",
        "# Code Outline:\n",
        "1. Import the dataset.\n",
        "2. Import tokenizer/base model.\n",
        "3. Build dataset class.\n",
        "4. Build the main model architecture.\n",
        "5. Create the DataLoader.\n",
        "6. Split data into training and testing sets.\n",
        "7. Train the model.\n",
        "8. Test the model.\n",
        "9. Make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a18cd3b-fb23-4826-8be8-23d47c9d28c1",
      "metadata": {
        "id": "2a18cd3b-fb23-4826-8be8-23d47c9d28c1",
        "outputId": "9a09bf3a-133b-4df6-b67b-94cf6ce5c8e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/k_ensafitakaldani001_umb_edu/.conda/envs/MainProject/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#import the libraries:\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727e6a23-ce86-4a99-a80c-b95bbace0296",
      "metadata": {
        "id": "727e6a23-ce86-4a99-a80c-b95bbace0296",
        "outputId": "b0bc9dfd-0863-415f-9805-e8f84d4078fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Clear the CUDA memory cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#collect garbage to free up memory from unused objects\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75320e02-b7f6-4f8c-ad8e-3c7a48b48a7b",
      "metadata": {
        "id": "75320e02-b7f6-4f8c-ad8e-3c7a48b48a7b"
      },
      "source": [
        "# 1.Import the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6d8953-4c07-4c3f-9a30-91062a2fb0ca",
      "metadata": {
        "id": "af6d8953-4c07-4c3f-9a30-91062a2fb0ca"
      },
      "outputs": [],
      "source": [
        "#THIS SHOULD BE MODIFIED:  this is the path for the main dataset\n",
        "pairs_df = pd.read_csv('/home/k_ensafitakaldani001_umb_edu/Project1/merged1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394d8518-6c91-433c-90b5-4788ed0065f7",
      "metadata": {
        "id": "394d8518-6c91-433c-90b5-4788ed0065f7",
        "outputId": "47162cf6-2093-4aac-e273-efb850a0f797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44475\n",
            "40595\n",
            "0\n",
            "40595\n"
          ]
        }
      ],
      "source": [
        "#Just some cleaning up needed for dataset: No need to get to deep in this part\n",
        "\n",
        "#pairs_df.head(10)\n",
        "len(pairs_df)\n",
        "\n",
        "# Count the number of NaN values in the 'SASA_A' column\n",
        "nan_count = pairs_df['SASA_A'].isna().sum()\n",
        "\n",
        "print(nan_count)\n",
        "\n",
        "\n",
        "# Drop rows where the 'sasa_A' column has NaN values\n",
        "df_cleaned = pairs_df.dropna(subset=['SASA_A'])\n",
        "\n",
        "len(df_cleaned)\n",
        "\n",
        "pairs_df = df_cleaned\n",
        "\n",
        "len(pairs_df)\n",
        "\n",
        "df = pairs_df\n",
        "filtered_df = df[(df['Sequence_A'].str.len() >= 50) & (df['Sequence_A'].str.len() < 200)]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df\n",
        "\n",
        "\n",
        "# Count the number of NaN values in the 'SASA_A' column\n",
        "nan_count = pairs_df['SASA_A'].isna().sum()\n",
        "\n",
        "print(nan_count)\n",
        "\n",
        "df = pairs_df\n",
        "filtered_df = df[(df['Sequence_A'].str.len() >= 50) & (df['Sequence_A'].str.len() < 200)]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e8c201-5cbc-4577-a12d-b65b1701041d",
      "metadata": {
        "id": "65e8c201-5cbc-4577-a12d-b65b1701041d"
      },
      "source": [
        "# 2.Import tokenizer/base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db1d2f2-38c5-4629-9696-75bed9202998",
      "metadata": {
        "id": "0db1d2f2-38c5-4629-9696-75bed9202998",
        "outputId": "711af915-b7c9-4033-ee32-3cf27e1b47bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token '[ENTITY1]' has ID: 30\n",
            "Token '[ENTITY2]' has ID: 31\n",
            "Token '-' has ID: 32\n",
            "Token 'BR' has ID: 33\n",
            "Token 'PE' has ID: 34\n",
            "Token 'EX' has ID: 35\n",
            "Token embeddings resized to accommodate new tokens.\n",
            "Updated vocabulary size: 36\n",
            "All special tokens are in the tokenizer's vocabulary.\n",
            "36\n",
            "36\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import BertModel, BertConfig, AutoTokenizer\n",
        "\n",
        "\n",
        "'''\n",
        "Initialize the ProtBERT tokenizer and model -> mainly use these for pretraining\n",
        "used for a variety of downstream tasks (e.g., classification, tagging).\n",
        "Unlike AutoModelForMaskedLM, it is not specifically tied to masked language modeling\n",
        "'''\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "\n",
        "# Define special tokens for entities -> our new tokens for seperation and sasa classification\n",
        "special_tokens = ['[ENTITY1]', '[ENTITY2]', '-', 'BR', 'PE', 'EX']\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Check if the special tokens were added successfully\n",
        "print(f\"Token '[ENTITY1]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY1]')}\")\n",
        "print(f\"Token '[ENTITY2]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY2]')}\")\n",
        "print(f\"Token '-' has ID: {tokenizer.convert_tokens_to_ids('-')}\")\n",
        "print(f\"Token 'BR' has ID: {tokenizer.convert_tokens_to_ids('BR')}\")\n",
        "print(f\"Token 'PE' has ID: {tokenizer.convert_tokens_to_ids('PE')}\")\n",
        "print(f\"Token 'EX' has ID: {tokenizer.convert_tokens_to_ids('EX')}\")\n",
        "\n",
        "# Resize the model's embedding size to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print('Token embeddings resized to accommodate new tokens.')\n",
        "\n",
        "# Helper function to convert numerical token IDs back to their textual representation\n",
        "def ids_to_text(ids):\n",
        "    return ' '.join(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "# Check the updated size of the tokenizer's vocabulary\n",
        "print(f\"Updated vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "# Check if the new tokens are in the tokenizer's vocabulary\n",
        "if all(token in tokenizer.get_vocab() for token in special_tokens):\n",
        "    print(\"All special tokens are in the tokenizer's vocabulary.\")\n",
        "else:\n",
        "    print(\"Some special tokens are NOT in the tokenizer's vocabulary.\")\n",
        "\n",
        "\n",
        "#some checks:\n",
        "vocab= tokenizer.get_vocab()\n",
        "print(len(vocab))\n",
        "\n",
        "# Get the number of amino acids\n",
        "num_amino_acids = len(tokenizer.get_vocab())\n",
        "print(num_amino_acids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c61ac47-78d0-4c90-93c3-ea367eda1410",
      "metadata": {
        "id": "4c61ac47-78d0-4c90-93c3-ea367eda1410"
      },
      "source": [
        "#  3.Build dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c73f08a-6ad6-484f-9a2f-76ed99cbbdd3",
      "metadata": {
        "id": "8c73f08a-6ad6-484f-9a2f-76ed99cbbdd3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_SASA_sequence(self, row):\n",
        "        SASA_A = row['SASA_A']\n",
        "        SASA_B = row['SASA_B']\n",
        "\n",
        "        try:\n",
        "            if isinstance(SASA_A, str):\n",
        "                SASA_A = SASA_A.split(\", \")\n",
        "            elif not isinstance(SASA_A, list):\n",
        "                SASA_A = [str(SASA_A)]\n",
        "\n",
        "            if isinstance(SASA_B, str):\n",
        "                SASA_B = SASA_B.split(\", \")\n",
        "            elif not isinstance(SASA_B, list):\n",
        "                SASA_B = [str(SASA_B)]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing SASA sequences: {e}\")\n",
        "            print(f\"SASA_A: {SASA_A}, SASA_B: {SASA_B}\")\n",
        "\n",
        "        SASA_sequence = f\"[ENTITY1] {' '.join(SASA_A)} [SEP] [ENTITY2] {' '.join(SASA_B)}\"\n",
        "        return SASA_sequence\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx]\n",
        "        Global_Sequence_A = row['Sequence_A']\n",
        "        Global_Sequence_B = row['Sequence_B']\n",
        "        Local_Sequence_A = row['masked_sequence_A']\n",
        "        Local_Sequence_B = row['masked_sequence_B']\n",
        "\n",
        "        # Enhanced sequences with new tokens\n",
        "        Global_sequence = f\"[ENTITY1] {Global_Sequence_A} [SEP] [ENTITY2] {Global_Sequence_B}\"\n",
        "        Local_sequence = f\"[ENTITY1] {Local_Sequence_A} [SEP] [ENTITY2] {Local_Sequence_B}\"\n",
        "        SASA_sequence = self.get_SASA_sequence(row)\n",
        "\n",
        "        # Tokenize input, label, and SASA sequences\n",
        "        global_inputs = self.tokenizer(Global_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        labels = self.tokenizer(Local_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        SASA_inputs = self.tokenizer(SASA_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "\n",
        "        # Ensure all outputs are properly returned\n",
        "        return {\n",
        "            'global_input_ids': global_inputs['input_ids'].squeeze(0),\n",
        "            'global_attention_mask': global_inputs['attention_mask'].squeeze(0),\n",
        "            'SASA_input_ids': SASA_inputs['input_ids'].squeeze(0),  # SASA tokenized and part of input\n",
        "            'SASA_attention_mask': SASA_inputs['attention_mask'].squeeze(0),  # Attention mask for SASA inputs\n",
        "            'labels': labels['input_ids'].squeeze(0)  # Local sequences as labels\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs = {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd3abf6-eccc-4891-be40-660dda75ff98",
      "metadata": {
        "id": "3dd3abf6-eccc-4891-be40-660dda75ff98"
      },
      "source": [
        "# 4. Build the main model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b0d260-6883-4d1c-8d1c-dc337fcb6f3f",
      "metadata": {
        "id": "a0b0d260-6883-4d1c-8d1c-dc337fcb6f3f"
      },
      "outputs": [],
      "source": [
        "# Gated Mechanism with Improvements\n",
        "\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_amino_acids = num_amino_acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Improved Gating mechanism using MLP\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(model.config.hidden_size * 2, model.config.hidden_size),\n",
        "            nn.ReLU(),  # ReLU for non-linearity\n",
        "            nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
        "            nn.Sigmoid()  # Can experiment with other activations\n",
        "        )\n",
        "\n",
        "        # Dynamic learnable scalars for global and SASA outputs\n",
        "        self.alpha = nn.Parameter(torch.randn(1))  # Dynamically learnable alpha\n",
        "        self.beta = nn.Parameter(torch.randn(1))   # Dynamically learnable beta\n",
        "\n",
        "        # Incorporate multi-head attention for combining global and SASA outputs\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=model.config.hidden_size, num_heads=4)\n",
        "\n",
        "        # Adding layer normalization to stabilize training\n",
        "        self.layer_norm = nn.LayerNorm(model.config.hidden_size)\n",
        "\n",
        "        # Option for residual connections\n",
        "        self.use_residual = True  # Option to use residual connections\n",
        "\n",
        "        # Classifier for prediction\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_amino_acids)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Multi-head attention over global and SASA outputs\n",
        "        attention_output, _ = self.attention(global_outputs, SASA_outputs, SASA_outputs)\n",
        "\n",
        "        # Concatenate global and SASA outputs with attention output\n",
        "        combined_inputs = torch.cat((attention_output, SASA_outputs), dim=-1)\n",
        "\n",
        "        # Apply improved gating mechanism (MLP-based)\n",
        "        gate_output = self.gate_mlp(combined_inputs)\n",
        "        mixed_outputs = gate_output * (self.alpha * global_outputs) + (1 - gate_output) * (self.beta * SASA_outputs)\n",
        "\n",
        "        # Apply residual connections if enabled\n",
        "        if self.use_residual:\n",
        "            mixed_outputs += global_outputs + SASA_outputs  # Residual connections\n",
        "\n",
        "        # Apply layer normalization to stabilize the mixed outputs\n",
        "        mixed_outputs = self.layer_norm(mixed_outputs)\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(mixed_outputs)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2cc8ee0-f0d1-4e35-b82d-fd327457451f",
      "metadata": {
        "tags": [],
        "id": "a2cc8ee0-f0d1-4e35-b82d-fd327457451f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This section is still part of building the model's main architecture.\n",
        "We're just defining the training and evaluation functions.\n",
        "'''\n",
        "\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# Function to save checkpoint -> DONT FORGET TO MODIFY THE PATH IN YOUR SYSTEM\n",
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# Training function with updated autocast and GradScaler\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs, accumulation_steps=2, checkpoint_path=\"checkpoint240.pth.tar\"):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    start_epoch = 0\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "    scaler = GradScaler()  # Updated mixed precision scaler\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss_history = checkpoint['loss_history']\n",
        "        val_loss_history = checkpoint.get('val_loss_history', [])\n",
        "        val_accuracy_history = checkpoint.get('val_accuracy_history', [])\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        total_loss = 0\n",
        "        model.train()  # Ensure model is in training mode\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Updated autocast to torch.amp.autocast\n",
        "            with autocast():\n",
        "                outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "                loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1)) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()  # Scaled backward pass\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:  # Perform optimizer step every few batches\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps  # Accumulate the loss\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}')\n",
        "\n",
        "            # Freeing memory after each batch\n",
        "            del global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask, labels, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        loss_history.append(average_loss)\n",
        "        print(f'End of Epoch {epoch + 1}, Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # --- Evaluate the model on the validation set after each epoch ---\n",
        "        avg_val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "        print(f'Epoch {epoch + 1} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "        # Save checkpoint after every epoch\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss_history': loss_history,\n",
        "            'val_loss_history': val_loss_history,\n",
        "            'val_accuracy_history': val_accuracy_history\n",
        "        }, filename=checkpoint_path)\n",
        "\n",
        "    return loss_history, val_loss_history, val_accuracy_history\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_labels = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during evaluation\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Use autocast for mixed precision evaluation\n",
        "            with autocast():\n",
        "                outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "                loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # For accuracy calculation (optional, depending on your task)\n",
        "            predictions = torch.argmax(outputs, dim=-1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_labels += labels.numel()\n",
        "\n",
        "            # Free memory after each batch\n",
        "            del global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask, labels, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_labels if total_labels > 0 else 0\n",
        "\n",
        "    print(f'Evaluation completed, Average Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    return average_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1d2477-38a8-4cf0-b466-fe35718da2eb",
      "metadata": {
        "id": "7b1d2477-38a8-4cf0-b466-fe35718da2eb"
      },
      "source": [
        "# 5. Create the DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24f784e-f20a-4654-8cad-00577e9fcd84",
      "metadata": {
        "id": "d24f784e-f20a-4654-8cad-00577e9fcd84",
        "outputId": "26a44b61-5ac6-4e00-9ddf-95038e99ea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset size: 40595\n",
            "Training data size: 32476\n",
            "Number of batches in train_loader: 2030, Each batch has 16 samples.\n",
            "Number of batches in val_loader: 254, Each batch has 16 samples.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split #split the train and test\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "train_df, val_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Manually reduce the validation DataFrame size by taking 20% of the original validation set\n",
        "val_df = val_df.sample(frac=0.5, random_state=42)  # Keep 20% of the original validation split\n",
        "\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "\n",
        "# Setup DataLoaders for training and validation\n",
        "train_dataset = SampleDataset(train_df, tokenizer,500)\n",
        "val_dataset = SampleDataset(val_df, tokenizer, 500)\n",
        "\n",
        "# Using pin_memory for faster host to device transfer\n",
        "# Increasing num_workers to use multiple CPU cores for data loading\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn,  num_workers=2, pin_memory=True)\n",
        "\n",
        "# Log the setup\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}, Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}, Each batch has {val_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40453568-2239-4b7e-98ca-8fd941e8d56d",
      "metadata": {
        "id": "40453568-2239-4b7e-98ca-8fd941e8d56d"
      },
      "source": [
        "# 7. Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492001a8-4bca-40b0-884d-a370f39a51b4",
      "metadata": {
        "id": "492001a8-4bca-40b0-884d-a370f39a51b4"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam, lr_scheduler\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "SEQ_LEN = 500\n",
        "LR_STEP_SIZE = 2\n",
        "LR_GAMMA = 0.5\n",
        "ACCUMULATION_STEPS = 2\n",
        "NUM_EPOCHS = 10\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = tokenizer\n",
        "\n",
        "# Load the base BertModel or similar from transformers suited for your needs\n",
        "base_model = model  # Replace with your base model\n",
        "\n",
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(vocab)  # Adjust this if your task isn't directly classification\n",
        "\n",
        "# Initialize your custom model with the base model\n",
        "my_model = ProtBertSeq2Seq(model=base_model, num_amino_acids=num_amino_acids, seq_len=500)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#lora_model.to(device)\n",
        "\n",
        "my_model.to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = Adam(my_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)\n",
        "\n",
        "# Loss func\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682292ce-7979-41d8-a3ca-f43babf82e63",
      "metadata": {
        "id": "682292ce-7979-41d8-a3ca-f43babf82e63",
        "outputId": "a6d4413f-7ade-4f78-a3d2-03926124a265"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3252321/3829059307.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Updated mixed precision scaler\n",
            "/tmp/ipykernel_3252321/3829059307.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded checkpoint 'checkpoint240.pth.tar' (epoch 10)\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Call the training function, which will also evaluate after each epoch\n",
        "train_loss_history, val_loss_history, val_accuracy_history = train_model(\n",
        "    my_model,  # Using the LoRA model\n",
        "    train_loader,  # Training data loader\n",
        "    val_loader,  # Validation data loader\n",
        "    optimizer,  # Optimizer for LoRA model\n",
        "    criterion,  # Loss function\n",
        "    device,  # Device (CPU or GPU)\n",
        "    num_epochs  # Number of epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f20bfa0-faec-4ee0-b312-6694e2dc5bd7",
      "metadata": {
        "id": "7f20bfa0-faec-4ee0-b312-6694e2dc5bd7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:.conda-MainProject]",
      "language": "python",
      "name": "conda-env-.conda-MainProject-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}