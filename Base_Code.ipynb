{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/SASA-Calculation-For-LLMs/blob/main/Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkG-WemRazLx"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36P5XPm9bLhe"
      },
      "source": [
        "# Agenda for the code:\n",
        "just give the global seq as input and local as label to the model without adding new tokens just do a prediction task simple as this then build up\n",
        "\n",
        "\n",
        "\n",
        "#outline of the code:\n",
        "import the dataset\n",
        "\n",
        "import tokenizer / base model\n",
        "\n",
        "build dataset class\n",
        "\n",
        "make the dataloader\n",
        "\n",
        "split the test and training data\n",
        "\n",
        "train\n",
        "\n",
        "test\n",
        "\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWSbp4XSbBi4"
      },
      "outputs": [],
      "source": [
        "pairs_df = pd.read_csv('/home/k_ensafitakaldani001_umb_edu/Project1/pairs_df_with_sasa.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "iwBmfF-jczgH",
        "outputId": "873a752b-9271-42d2-8e32-ef7a937d3e96"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>Protein Name A</th>\n",
              "      <th>Protein Name B</th>\n",
              "      <th>masked_sequence_A</th>\n",
              "      <th>masked_sequence_B</th>\n",
              "      <th>Embeddings_A</th>\n",
              "      <th>Sequence_A</th>\n",
              "      <th>Embeddings_B</th>\n",
              "      <th>Sequence_B</th>\n",
              "      <th>tokenized_sequence_A</th>\n",
              "      <th>tokenized_sequence_B</th>\n",
              "      <th>tokenized_masked_sequence_A</th>\n",
              "      <th>tokenized_masked_sequence_B</th>\n",
              "      <th>sum_tokenized_sequence_A</th>\n",
              "      <th>sum_tokenized_sequence_B</th>\n",
              "      <th>SASA_Category_A</th>\n",
              "      <th>SASA_Category_B</th>\n",
              "      <th>SASA_A</th>\n",
              "      <th>SASA_B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3TXS</td>\n",
              "      <td>3TXS_C</td>\n",
              "      <td>3TXS_B</td>\n",
              "      <td>-----L-LRDP--NPN-----Q--DY--V--NM--Q--M---M---...</td>\n",
              "      <td>----------------------D--YE--RR--HY------D----...</td>\n",
              "      <td>tensor([-0.3274, -1.2364, -2.2224, -1.0129,  0...</td>\n",
              "      <td>QVYAPLVLRDPVSNPNNRKIDQDDDYELVRRNMHYQSQMLLDMAKI...</td>\n",
              "      <td>tensor([ 2.1163, -0.8901,  0.5806,  0.6658,  1...</td>\n",
              "      <td>QVYAPLVLRDPVSNPNNRKIDQDDDYELVRRNMHYQSQMLLDMAKI...</td>\n",
              "      <td>[18, 8, 20, 6, 16, 5, 8, 5, 13, 14, 16, 8, 10,...</td>\n",
              "      <td>[18, 8, 20, 6, 16, 5, 8, 5, 13, 14, 16, 8, 10,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 5, 32, 5, 13, 14, 16, 32,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[50, 40, 52, 38, 48, 10, 40, 10, 26, 28, 32, 4...</td>\n",
              "      <td>[50, 40, 52, 38, 48, 37, 40, 37, 45, 46, 48, 4...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3TY1</td>\n",
              "      <td>3TY1_B</td>\n",
              "      <td>3TY1_C</td>\n",
              "      <td>--------------------------------------VLHGGKQ-...</td>\n",
              "      <td>--------NAE-GI--------------------------------...</td>\n",
              "      <td>tensor([-4.0670e-01,  2.4104e+00, -1.7887e-01,...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>tensor([-0.8169, -1.0578, -0.4143,  0.3436,  0...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 17, 6, 9, 32,...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 47, 34, 12, 18, 4...</td>\n",
              "      <td>BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>90.55, 51.82, 37.88, 66.74, 61.6, 52.58, 0.0, ...</td>\n",
              "      <td>82.67, 62.54, 108.26, 65.32, 33.83, 52.08, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3TY1</td>\n",
              "      <td>3TY1_B</td>\n",
              "      <td>3TY1_A</td>\n",
              "      <td>-------T-AE-GI--------------------------------...</td>\n",
              "      <td>---------------------------------------LHGGKQ-...</td>\n",
              "      <td>tensor([-4.0670e-01,  2.4104e+00, -1.7887e-01,...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>tensor([-0.4314,  0.0235,  2.0023, -1.0242, -1...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 15, 32, 6, 9, 32,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 30, 49, 12, 18, 4...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...</td>\n",
              "      <td>BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>90.55, 51.82, 37.88, 66.74, 61.6, 52.58, 0.0, ...</td>\n",
              "      <td>88.2, 54.31, 125.05, 59.95, 37.55, 53.99, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3TY1</td>\n",
              "      <td>3TY1_C</td>\n",
              "      <td>3TY1_A</td>\n",
              "      <td>---------------------------------------LHGGKQ-...</td>\n",
              "      <td>--------NAE-GI--------------------------------...</td>\n",
              "      <td>tensor([-0.8169, -1.0578, -0.4143,  0.3436,  0...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>tensor([-0.4314,  0.0235,  2.0023, -1.0242, -1...</td>\n",
              "      <td>GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 17, 6, 9, 32,...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...</td>\n",
              "      <td>[39, 38, 44, 47, 56, 40, 37, 47, 34, 12, 18, 4...</td>\n",
              "      <td>BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...</td>\n",
              "      <td>82.67, 62.54, 108.26, 65.32, 33.83, 52.08, 0.0...</td>\n",
              "      <td>88.2, 54.31, 125.05, 59.95, 37.55, 53.99, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3TY2</td>\n",
              "      <td>3TY2_B</td>\n",
              "      <td>3TY2_A</td>\n",
              "      <td>---------------------------------------SGASNSL...</td>\n",
              "      <td>---------------------------------------SGASNSL...</td>\n",
              "      <td>tensor([-9.0190e-01,  4.1558e-01,  2.0426e+00,...</td>\n",
              "      <td>KLRLLLSNDDGVYAKGLAILAKTLADLGEVDVVAPDRNRSGASNSL...</td>\n",
              "      <td>tensor([-2.3793e-01,  1.4242e-01, -2.7027e-03,...</td>\n",
              "      <td>KLRLLLSNDDGVYAKGLAILAKTLADLGEVDVVAPDRNRSGASNSL...</td>\n",
              "      <td>[12, 5, 13, 5, 5, 5, 10, 17, 14, 14, 7, 8, 20,...</td>\n",
              "      <td>[12, 5, 13, 5, 5, 5, 10, 17, 14, 14, 7, 8, 20,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[44, 37, 45, 37, 37, 37, 42, 49, 46, 46, 39, 4...</td>\n",
              "      <td>[44, 37, 45, 37, 37, 37, 42, 49, 46, 46, 39, 4...</td>\n",
              "      <td>PE, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR...</td>\n",
              "      <td>PE, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR...</td>\n",
              "      <td>179.71, 19.22, 28.27, 12.27, 0.16, 0.0, 0.03, ...</td>\n",
              "      <td>193.01, 35.39, 49.86, 9.8, 0.16, 0.0, 0.07, 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  pair_id Protein Name A Protein Name B  \\\n",
              "0    3TXS         3TXS_C         3TXS_B   \n",
              "1    3TY1         3TY1_B         3TY1_C   \n",
              "2    3TY1         3TY1_B         3TY1_A   \n",
              "3    3TY1         3TY1_C         3TY1_A   \n",
              "4    3TY2         3TY2_B         3TY2_A   \n",
              "\n",
              "                                   masked_sequence_A  \\\n",
              "0  -----L-LRDP--NPN-----Q--DY--V--NM--Q--M---M---...   \n",
              "1  --------------------------------------VLHGGKQ-...   \n",
              "2  -------T-AE-GI--------------------------------...   \n",
              "3  ---------------------------------------LHGGKQ-...   \n",
              "4  ---------------------------------------SGASNSL...   \n",
              "\n",
              "                                   masked_sequence_B  \\\n",
              "0  ----------------------D--YE--RR--HY------D----...   \n",
              "1  --------NAE-GI--------------------------------...   \n",
              "2  ---------------------------------------LHGGKQ-...   \n",
              "3  --------NAE-GI--------------------------------...   \n",
              "4  ---------------------------------------SGASNSL...   \n",
              "\n",
              "                                        Embeddings_A  \\\n",
              "0  tensor([-0.3274, -1.2364, -2.2224, -1.0129,  0...   \n",
              "1  tensor([-4.0670e-01,  2.4104e+00, -1.7887e-01,...   \n",
              "2  tensor([-4.0670e-01,  2.4104e+00, -1.7887e-01,...   \n",
              "3  tensor([-0.8169, -1.0578, -0.4143,  0.3436,  0...   \n",
              "4  tensor([-9.0190e-01,  4.1558e-01,  2.0426e+00,...   \n",
              "\n",
              "                                          Sequence_A  \\\n",
              "0  QVYAPLVLRDPVSNPNNRKIDQDDDYELVRRNMHYQSQMLLDMAKI...   \n",
              "1  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "2  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "3  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "4  KLRLLLSNDDGVYAKGLAILAKTLADLGEVDVVAPDRNRSGASNSL...   \n",
              "\n",
              "                                        Embeddings_B  \\\n",
              "0  tensor([ 2.1163, -0.8901,  0.5806,  0.6658,  1...   \n",
              "1  tensor([-0.8169, -1.0578, -0.4143,  0.3436,  0...   \n",
              "2  tensor([-0.4314,  0.0235,  2.0023, -1.0242, -1...   \n",
              "3  tensor([-0.4314,  0.0235,  2.0023, -1.0242, -1...   \n",
              "4  tensor([-2.3793e-01,  1.4242e-01, -2.7027e-03,...   \n",
              "\n",
              "                                          Sequence_B  \\\n",
              "0  QVYAPLVLRDPVSNPNNRKIDQDDDYELVRRNMHYQSQMLLDMAKI...   \n",
              "1  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "2  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "3  GAKTWVLTNAEEGIDKGNWQINSDQLKVKDHAFSIEQKVLHGGKQE...   \n",
              "4  KLRLLLSNDDGVYAKGLAILAKTLADLGEVDVVAPDRNRSGASNSL...   \n",
              "\n",
              "                                tokenized_sequence_A  \\\n",
              "0  [18, 8, 20, 6, 16, 5, 8, 5, 13, 14, 16, 8, 10,...   \n",
              "1  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "2  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "3  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "4  [12, 5, 13, 5, 5, 5, 10, 17, 14, 14, 7, 8, 20,...   \n",
              "\n",
              "                                tokenized_sequence_B  \\\n",
              "0  [18, 8, 20, 6, 16, 5, 8, 5, 13, 14, 16, 8, 10,...   \n",
              "1  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "2  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "3  [7, 6, 12, 15, 24, 8, 5, 15, 17, 6, 9, 9, 7, 1...   \n",
              "4  [12, 5, 13, 5, 5, 5, 10, 17, 14, 14, 7, 8, 20,...   \n",
              "\n",
              "                         tokenized_masked_sequence_A  \\\n",
              "0  [32, 32, 32, 32, 32, 5, 32, 5, 13, 14, 16, 32,...   \n",
              "1  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "2  [32, 32, 32, 32, 32, 32, 32, 15, 32, 6, 9, 32,...   \n",
              "3  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "4  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                         tokenized_masked_sequence_B  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "1  [32, 32, 32, 32, 32, 32, 32, 32, 17, 6, 9, 32,...   \n",
              "2  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "3  [32, 32, 32, 32, 32, 32, 32, 32, 17, 6, 9, 32,...   \n",
              "4  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                            sum_tokenized_sequence_A  \\\n",
              "0  [50, 40, 52, 38, 48, 10, 40, 10, 26, 28, 32, 4...   \n",
              "1  [39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...   \n",
              "2  [39, 38, 44, 47, 56, 40, 37, 30, 49, 12, 18, 4...   \n",
              "3  [39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...   \n",
              "4  [44, 37, 45, 37, 37, 37, 42, 49, 46, 46, 39, 4...   \n",
              "\n",
              "                            sum_tokenized_sequence_B  \\\n",
              "0  [50, 40, 52, 38, 48, 37, 40, 37, 45, 46, 48, 4...   \n",
              "1  [39, 38, 44, 47, 56, 40, 37, 47, 34, 12, 18, 4...   \n",
              "2  [39, 38, 44, 47, 56, 40, 37, 47, 49, 38, 41, 4...   \n",
              "3  [39, 38, 44, 47, 56, 40, 37, 47, 34, 12, 18, 4...   \n",
              "4  [44, 37, 45, 37, 37, 37, 42, 49, 46, 46, 39, 4...   \n",
              "\n",
              "                                     SASA_Category_A  \\\n",
              "0                                                NaN   \n",
              "1  BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "2  BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "3  BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "4  PE, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR...   \n",
              "\n",
              "                                     SASA_Category_B  \\\n",
              "0                                                NaN   \n",
              "1  BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "2  BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "3  BR, BR, PE, BR, BR, BR, BR, BR, BR, BR, PE, PE...   \n",
              "4  PE, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR...   \n",
              "\n",
              "                                              SASA_A  \\\n",
              "0                                                NaN   \n",
              "1  90.55, 51.82, 37.88, 66.74, 61.6, 52.58, 0.0, ...   \n",
              "2  90.55, 51.82, 37.88, 66.74, 61.6, 52.58, 0.0, ...   \n",
              "3  82.67, 62.54, 108.26, 65.32, 33.83, 52.08, 0.0...   \n",
              "4  179.71, 19.22, 28.27, 12.27, 0.16, 0.0, 0.03, ...   \n",
              "\n",
              "                                              SASA_B  \n",
              "0                                                NaN  \n",
              "1  82.67, 62.54, 108.26, 65.32, 33.83, 52.08, 0.0...  \n",
              "2  88.2, 54.31, 125.05, 59.95, 37.55, 53.99, 0.0,...  \n",
              "3  88.2, 54.31, 125.05, 59.95, 37.55, 53.99, 0.0,...  \n",
              "4  193.01, 35.39, 49.86, 9.8, 0.16, 0.0, 0.07, 0....  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAvCVYi0b9O-"
      },
      "source": [
        "dataset:\n",
        "first -> make 1 input from seq A and B\n",
        "sencond -> make masked seq A and B as their labels\n",
        "\n",
        "goal: make a model that can predict the local sequences of A and B based on global sequences A and B.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "inputs:\n",
        "[Entity 1] abweregfvkk [Entity 2] qwmfdefjlxcvg\n",
        "\n",
        "predictions: predictions should be something like:\n",
        "[Entity 1] ab---egf--- [Entity 2] ---fde---xcvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uXfKGHYnMEx",
        "outputId": "21a5a249-d368-41c2-c37b-518daced0908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17553\n"
          ]
        }
      ],
      "source": [
        "df = pairs_df\n",
        "filtered_df = df[df['Sequence_A'].str.len() >= 50]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU_Rpwmj1AcB",
        "outputId": "0d5985fe-a05a-4c3e-d944-4ff31854abde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token '[ENTITY1]' has ID: 30\n",
            "Token '[ENTITY2]' has ID: 31\n",
            "Token '-' has ID: 32\n",
            "Token 'BR' has ID: 33\n",
            "Token 'PE' has ID: 34\n",
            "Token 'EX' has ID: 35\n",
            "Token embeddings resized to accommodate new tokens.\n",
            "Updated vocabulary size: 36\n",
            "All special tokens are in the tokenizer's vocabulary.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import BertModel, BertConfig, AutoTokenizer\n",
        "\n",
        "# Initialize the ProtBERT tokenizer and model -> mainly use these for pretraining\n",
        "\n",
        "# used for a variety of downstream tasks (e.g., classification, tagging). Unlike AutoModelForMaskedLM, it is not specifically tied to masked language modeling\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "\n",
        "# Define special tokens for entities\n",
        "special_tokens = ['[ENTITY1]', '[ENTITY2]', '-', 'BR', 'PE', 'EX']\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Check if the special tokens were added successfully\n",
        "print(f\"Token '[ENTITY1]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY1]')}\")\n",
        "print(f\"Token '[ENTITY2]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY2]')}\")\n",
        "print(f\"Token '-' has ID: {tokenizer.convert_tokens_to_ids('-')}\")\n",
        "print(f\"Token 'BR' has ID: {tokenizer.convert_tokens_to_ids('BR')}\")\n",
        "print(f\"Token 'PE' has ID: {tokenizer.convert_tokens_to_ids('PE')}\")\n",
        "print(f\"Token 'EX' has ID: {tokenizer.convert_tokens_to_ids('EX')}\")\n",
        "\n",
        "# Resize the model's embedding size to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print('Token embeddings resized to accommodate new tokens.')\n",
        "\n",
        "# Helper function to convert numerical token IDs back to their textual representation\n",
        "def ids_to_text(ids):\n",
        "    return ' '.join(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "# Check the updated size of the tokenizer's vocabulary\n",
        "print(f\"Updated vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "# Check if the new tokens are in the tokenizer's vocabulary\n",
        "if all(token in tokenizer.get_vocab() for token in special_tokens):\n",
        "    print(\"All special tokens are in the tokenizer's vocabulary.\")\n",
        "else:\n",
        "    print(\"Some special tokens are NOT in the tokenizer's vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uic0UPfg6nhS"
      },
      "outputs": [],
      "source": [
        "vocab= tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0uacbmd6sAi",
        "outputId": "a801b504-29e2-4001-f3a2-e1f27e0ba9e9",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEqPZlLZvKGV",
        "outputId": "a4c63f89-5442-4e9f-8aa3-2b337eeb400d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        }
      ],
      "source": [
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(tokenizer.get_vocab())  # Adjust this if your task isn't directly classification\n",
        "print(num_amino_acids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEgjaqlqO7IC"
      },
      "outputs": [],
      "source": [
        "#This is a vertsion which we dont need now\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_SASA_sequence(self, row):\n",
        "        SASA_A = row['SASA_Category_A']\n",
        "        SASA_B = row['SASA_Category_B']\n",
        "\n",
        "        try:\n",
        "            if isinstance(SASA_A, str):\n",
        "                SASA_A = SASA_A.split(\", \")\n",
        "            elif not isinstance(SASA_A, list):\n",
        "                SASA_A = [str(SASA_A)]\n",
        "\n",
        "            if isinstance(SASA_B, str):\n",
        "                SASA_B = SASA_B.split(\", \")\n",
        "            elif not isinstance(SASA_B, list):\n",
        "                SASA_B = [str(SASA_B)]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing SASA sequences: {e}\")\n",
        "            print(f\"SASA_A: {SASA_A}, SASA_B: {SASA_B}\")\n",
        "\n",
        "        SASA_sequence = f\"[ENTITY1] {' '.join(SASA_A)} [SEP] [ENTITY2] {' '.join(SASA_B)}\"\n",
        "        return SASA_sequence\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx]\n",
        "        Global_Sequence_A = row['Sequence_A']\n",
        "        Global_Sequence_B = row['Sequence_B']\n",
        "        Local_Sequence_A = row['masked_sequence_A']\n",
        "        Local_Sequence_B = row['masked_sequence_B']\n",
        "\n",
        "        # Enhanced sequences with new tokens\n",
        "        Global_sequence = f\"[ENTITY1] {Global_Sequence_A} [SEP] [ENTITY2] {Global_Sequence_B}\"\n",
        "        Local_sequence = f\"[ENTITY1] {Local_Sequence_A} [SEP] [ENTITY2] {Local_Sequence_B}\"\n",
        "        SASA_sequence = self.get_SASA_sequence(row)\n",
        "\n",
        "        # Tokenize input, label, and SASA sequences\n",
        "        global_inputs = self.tokenizer(Global_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        labels = self.tokenizer(Local_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        SASA_inputs = self.tokenizer(SASA_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "\n",
        "        # Ensure all outputs are properly returned\n",
        "        return {\n",
        "            'global_input_ids': global_inputs['input_ids'].squeeze(0),\n",
        "            'global_attention_mask': global_inputs['attention_mask'].squeeze(0),\n",
        "            'SASA_input_ids': SASA_inputs['input_ids'].squeeze(0),  # SASA tokenized and part of input\n",
        "            'SASA_attention_mask': SASA_inputs['attention_mask'].squeeze(0),  # Attention mask for SASA inputs\n",
        "            'labels': labels['input_ids'].squeeze(0)  # Local sequences as labels\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs = {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fanKkNHFdzHS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model  # The underlying ProtBERT model\n",
        "        self.seq_len = seq_len  # Sequence length\n",
        "        self.num_amino_acids = num_amino_acids  # Total number of possible amino acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer to prevent overfitting\n",
        "        # Separate classifiers for each input might be considered depending on the model design\n",
        "        self.classifier = nn.Linear(model.config.hidden_size * 2, num_amino_acids)  # Classifier to predict amino acids based on concatenated outputs\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Combine the outputs from the two channels\n",
        "        # This simple concatenation can be replaced with more sophisticated methods (e.g., addition, averaging)\n",
        "        combined_outputs = torch.cat((global_outputs, SASA_outputs), dim=-1)\n",
        "\n",
        "        # Pass through the classifier to get predictions\n",
        "        logits = self.classifier(combined_outputs)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkSx2nzduaYb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(state, filename=\"/home/k_ensafitakaldani001_umb_edu/Project1/checkpoint240.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epochs, checkpoint_path=\"checkpoint240.pth.tar\"):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    start_epoch = 0\n",
        "    loss_history = []\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss_history = checkpoint['loss_history']\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            # Handle multiple channels of inputs\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass now takes multiple inputs\n",
        "            outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "            loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}')\n",
        "\n",
        "        average_loss = total_loss / len(dataloader)\n",
        "        loss_history.append(average_loss)\n",
        "        print(f'End of Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss_history': loss_history\n",
        "        }, filename=checkpoint_path)\n",
        "\n",
        "    return loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_kGPlLX8Oai",
        "outputId": "389d084c-5a0c-45aa-fb55-83ed45538369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset size: 17553\n",
            "Training data size: 14042\n",
            "Number of batches in train_loader: 3511, Each batch has 4 samples.\n",
            "Number of batches in val_loader: 439, Each batch has 8 samples.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#split the train and test here\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "train_df, val_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "\n",
        "# Setup DataLoaders for training and validation\n",
        "train_dataset = SampleDataset(train_df, tokenizer,500)\n",
        "val_dataset = SampleDataset(val_df, tokenizer, 500)\n",
        "\n",
        "\n",
        "# Using pin_memory for faster host to device transfer\n",
        "# Increasing num_workers to use multiple CPU cores for data loading\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn,  num_workers=2, pin_memory=True)\n",
        "\n",
        "# Log the setup\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}, Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}, Each batch has {val_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J21E7fYePtRK",
        "outputId": "69ce3e94-3e1d-46a9-d416-f9107fce1d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: peft in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from peft) (22.0)\n",
            "Requirement already satisfied: psutil in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from peft) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (2.4.1)\n",
            "Requirement already satisfied: transformers in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from peft) (0.25.0)\n",
            "Requirement already satisfied: filelock in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
            "Requirement already satisfied: requests in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.68)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /modules/apps/ood/jupyterlab-matlab/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/k_ensafitakaldani001_umb_edu/.local/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Im9Ou7YeAf",
        "outputId": "7efdbe4b-8b47-46d7-8f16-bc14acb0b1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 420994084\n",
            "Trainable parameters: 983040\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from transformers import AutoTokenizer, BertModel  # Ensure you are using BertModel\n",
        "from peft import LoraModel, LoraConfig\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = tokenizer\n",
        "\n",
        "# Load the base BertModel or similar from transformers suited for your needs\n",
        "base_model = model  # Replace with your base model\n",
        "\n",
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(vocab)  # Adjust this if your task isn't directly classification\n",
        "\n",
        "# Initialize your custom model with the base model\n",
        "my_model = ProtBertSeq2Seq(model=base_model, num_amino_acids=num_amino_acids, seq_len=500)\n",
        "\n",
        "# LoRA configuration\n",
        "config = LoraConfig(\n",
        "    task_type=\"SEQ_2_SEQ_LM\",  # Confirm this is your intended task\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"value\", \"key\"],  # Make sure these modules exist in your base model\n",
        "    lora_dropout=0.01\n",
        ")\n",
        "\n",
        "# Initialize LoRA model with the custom model and specify the adapter name if necessary\n",
        "lora_model = LoraModel(my_model, config, adapter_name=\"default\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lora_model.to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = Adam(lora_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Check parameters and setup\n",
        "print(f\"Total parameters: {sum(p.numel() for p in lora_model.parameters())}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZPJ8NNdYhWX",
        "tags": [],
        "outputId": "f19f7f70-8d39-404a-e655-0abedd039a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 3.5281636714935303\n",
            "Epoch 1, Batch 11, Loss: 2.253066062927246\n",
            "Epoch 1, Batch 21, Loss: 1.5889934301376343\n",
            "Epoch 1, Batch 31, Loss: 1.3937289714813232\n",
            "Epoch 1, Batch 41, Loss: 1.1544857025146484\n",
            "Epoch 1, Batch 51, Loss: 1.132966160774231\n",
            "Epoch 1, Batch 61, Loss: 1.0448579788208008\n",
            "Epoch 1, Batch 71, Loss: 0.9937829375267029\n",
            "Epoch 1, Batch 81, Loss: 0.9605238437652588\n",
            "Epoch 1, Batch 91, Loss: 1.0256900787353516\n",
            "Epoch 1, Batch 101, Loss: 0.9708887338638306\n",
            "Epoch 1, Batch 111, Loss: 0.9117063879966736\n",
            "Epoch 1, Batch 121, Loss: 0.8523914813995361\n",
            "Epoch 1, Batch 131, Loss: 0.849824070930481\n",
            "Epoch 1, Batch 141, Loss: 0.8960143327713013\n",
            "Epoch 1, Batch 151, Loss: 0.9249775409698486\n",
            "Epoch 1, Batch 161, Loss: 0.8466256856918335\n",
            "Epoch 1, Batch 171, Loss: 0.8070195913314819\n",
            "Epoch 1, Batch 181, Loss: 0.8047234416007996\n",
            "Epoch 1, Batch 191, Loss: 0.8666531443595886\n",
            "Epoch 1, Batch 201, Loss: 0.7811369895935059\n",
            "Epoch 1, Batch 211, Loss: 0.9241071343421936\n",
            "Epoch 1, Batch 221, Loss: 1.0225526094436646\n",
            "Epoch 1, Batch 231, Loss: 0.8568130135536194\n",
            "Epoch 1, Batch 241, Loss: 0.8703228831291199\n",
            "Epoch 1, Batch 251, Loss: 0.7407199144363403\n",
            "Epoch 1, Batch 261, Loss: 0.7531246542930603\n",
            "Epoch 1, Batch 271, Loss: 0.7632208466529846\n",
            "Epoch 1, Batch 281, Loss: 0.8120355010032654\n",
            "Epoch 1, Batch 291, Loss: 0.792905330657959\n",
            "Epoch 1, Batch 301, Loss: 0.7681859135627747\n",
            "Epoch 1, Batch 311, Loss: 0.8812214136123657\n",
            "Epoch 1, Batch 321, Loss: 0.7550921440124512\n",
            "Epoch 1, Batch 331, Loss: 0.7685256004333496\n",
            "Epoch 1, Batch 341, Loss: 0.8273447751998901\n",
            "Epoch 1, Batch 351, Loss: 0.8594343662261963\n",
            "Epoch 1, Batch 361, Loss: 0.8264017701148987\n",
            "Epoch 1, Batch 371, Loss: 0.7340444922447205\n",
            "Epoch 1, Batch 381, Loss: 0.7772557735443115\n",
            "Epoch 1, Batch 391, Loss: 0.8211846351623535\n",
            "Epoch 1, Batch 401, Loss: 0.7230101227760315\n",
            "Epoch 1, Batch 411, Loss: 0.7227178812026978\n",
            "Epoch 1, Batch 421, Loss: 0.8654808402061462\n",
            "Epoch 1, Batch 431, Loss: 0.7723037004470825\n",
            "Epoch 1, Batch 441, Loss: 0.8583870530128479\n",
            "Epoch 1, Batch 451, Loss: 0.7245993614196777\n",
            "Epoch 1, Batch 461, Loss: 0.8309038281440735\n",
            "Epoch 1, Batch 471, Loss: 0.7918226718902588\n",
            "Epoch 1, Batch 481, Loss: 0.7398198843002319\n",
            "Epoch 1, Batch 491, Loss: 0.7757856845855713\n",
            "Epoch 1, Batch 501, Loss: 0.7135286927223206\n",
            "Epoch 1, Batch 511, Loss: 0.7161253690719604\n",
            "Epoch 1, Batch 521, Loss: 0.7180001139640808\n",
            "Epoch 1, Batch 531, Loss: 1.1854784488677979\n",
            "Epoch 1, Batch 541, Loss: 0.837137758731842\n",
            "Epoch 1, Batch 551, Loss: 0.7158519625663757\n",
            "Epoch 1, Batch 561, Loss: 1.0226391553878784\n",
            "Epoch 1, Batch 571, Loss: 0.7942861318588257\n",
            "Epoch 1, Batch 581, Loss: 0.7151639461517334\n",
            "Epoch 1, Batch 591, Loss: 0.731448769569397\n",
            "Epoch 1, Batch 601, Loss: 0.7618779540061951\n",
            "Epoch 1, Batch 611, Loss: 0.7076802849769592\n",
            "Epoch 1, Batch 621, Loss: 0.8397833704948425\n",
            "Epoch 1, Batch 631, Loss: 0.7511264681816101\n",
            "Epoch 1, Batch 641, Loss: 0.9682983756065369\n",
            "Epoch 1, Batch 651, Loss: 0.7461869120597839\n",
            "Epoch 1, Batch 661, Loss: 0.9624747633934021\n",
            "Epoch 1, Batch 671, Loss: 0.7392135858535767\n",
            "Epoch 1, Batch 681, Loss: 0.9654157757759094\n",
            "Epoch 1, Batch 691, Loss: 0.7149628400802612\n",
            "Epoch 1, Batch 701, Loss: 0.7227815985679626\n",
            "Epoch 1, Batch 711, Loss: 0.7092077732086182\n",
            "Epoch 1, Batch 721, Loss: 0.7892774939537048\n",
            "Epoch 1, Batch 731, Loss: 0.6849415302276611\n",
            "Epoch 1, Batch 741, Loss: 0.7298531532287598\n",
            "Epoch 1, Batch 751, Loss: 0.7112663388252258\n",
            "Epoch 1, Batch 761, Loss: 0.7690054178237915\n",
            "Epoch 1, Batch 771, Loss: 0.8437760472297668\n",
            "Epoch 1, Batch 781, Loss: 0.6888559460639954\n",
            "Epoch 1, Batch 791, Loss: 0.7209422588348389\n",
            "Epoch 1, Batch 801, Loss: 1.0849504470825195\n",
            "Epoch 1, Batch 811, Loss: 0.7391355037689209\n",
            "Epoch 1, Batch 821, Loss: 0.744387149810791\n",
            "Epoch 1, Batch 831, Loss: 1.044213056564331\n",
            "Epoch 1, Batch 841, Loss: 0.725966215133667\n",
            "Epoch 1, Batch 851, Loss: 0.7099606394767761\n",
            "Epoch 1, Batch 861, Loss: 0.68851637840271\n",
            "Epoch 1, Batch 871, Loss: 0.7851459980010986\n",
            "Epoch 1, Batch 881, Loss: 0.7851728796958923\n",
            "Epoch 1, Batch 891, Loss: 0.7381672263145447\n",
            "Epoch 1, Batch 901, Loss: 0.6968411207199097\n",
            "Epoch 1, Batch 911, Loss: 0.7709294557571411\n",
            "Epoch 1, Batch 921, Loss: 0.7077994346618652\n",
            "Epoch 1, Batch 931, Loss: 0.7883055806159973\n",
            "Epoch 1, Batch 941, Loss: 0.6784510016441345\n",
            "Epoch 1, Batch 951, Loss: 0.7420554161071777\n",
            "Epoch 1, Batch 961, Loss: 0.7415761351585388\n",
            "Epoch 1, Batch 971, Loss: 0.7580408453941345\n",
            "Epoch 1, Batch 981, Loss: 0.7039886713027954\n",
            "Epoch 1, Batch 991, Loss: 0.781085729598999\n",
            "Epoch 1, Batch 1001, Loss: 0.6875837445259094\n",
            "Epoch 1, Batch 1011, Loss: 0.7890121340751648\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[20], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, device, epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_model(lora_model, train_loader, optimizer, criterion, device, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6zkbruInME1"
      },
      "outputs": [],
      "source": [
        "#Already saved loss for the training. the checkpoints are also saved\n",
        "\n",
        "#loss history for under 240\n",
        "loss_history =[1.2644726806950528,\n",
        " 1.242887128370729,\n",
        " 1.2066624065750828,\n",
        " 1.2070949814920082,\n",
        " 1.206244443510456,\n",
        " 1.2060478101701662,\n",
        " 1.205697778779031,\n",
        " 1.2053182708883476,\n",
        " 1.2058163822979422,\n",
        " 1.2062993284159562]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "MhuwIp6hYuvn",
        "outputId": "6f656e33-8f6f-4c14-d066-a45f8d845a1e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_history):\n",
        "    plt.plot(loss_history, label='Training Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'loss_history' is obtained from 'train_model' function after the model training is complete.\n",
        "plot_loss(loss_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WqOJe_5nME2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        dataloader (DataLoader): The DataLoader for the dataset to evaluate.\n",
        "        criterion (torch.nn.Module): The loss function used for evaluation.\n",
        "        device (torch.device): The device tensors are sent to (GPU or CPU).\n",
        "\n",
        "    Returns:\n",
        "        list, float, float: The loss history for each batch, and the average loss and accuracy over the evaluation dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "    loss_history_val = []  # List to store loss of each batch\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1))\n",
        "            loss_history_val.append(loss.item())  # Append the current loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert outputs to probabilities and then to predicted labels\n",
        "            probs = torch.softmax(outputs, dim=-1)\n",
        "            predictions = torch.argmax(probs, dim=-1)\n",
        "            correct = (predictions == labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_examples += labels.nelement()\n",
        "\n",
        "            if i % 10 == 0:  # Print every 10 batches\n",
        "                print(f'Batch {i+1}: Loss: {loss.item():.4f}, Accuracy: {correct/labels.nelement():.4f}')\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_examples\n",
        "    print(f'Finished evaluation - Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "    return loss_history_val, average_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU_39h_FnME3",
        "outputId": "045d7fc9-73c7-470c-8f96-b04a3cdadf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation...\n",
            "Batch 1: Loss: 1.2339, Accuracy: 0.6535\n",
            "Batch 11: Loss: 1.1810, Accuracy: 0.8758\n",
            "Batch 21: Loss: 1.1764, Accuracy: 0.7380\n",
            "Batch 31: Loss: 1.2086, Accuracy: 0.8458\n",
            "Batch 41: Loss: 1.1979, Accuracy: 0.6595\n",
            "Batch 51: Loss: 1.1945, Accuracy: 0.7558\n",
            "Batch 61: Loss: 1.1849, Accuracy: 0.8505\n",
            "Batch 71: Loss: 1.1878, Accuracy: 0.7315\n",
            "Batch 81: Loss: 1.2126, Accuracy: 0.6653\n",
            "Batch 91: Loss: 1.1950, Accuracy: 0.6388\n",
            "Batch 101: Loss: 1.2002, Accuracy: 0.7007\n",
            "Batch 111: Loss: 1.2012, Accuracy: 0.7508\n",
            "Batch 121: Loss: 1.1775, Accuracy: 0.6577\n",
            "Batch 131: Loss: 1.1992, Accuracy: 0.5290\n",
            "Batch 141: Loss: 1.1845, Accuracy: 0.7705\n",
            "Batch 151: Loss: 1.1726, Accuracy: 0.7192\n",
            "Batch 161: Loss: 1.2319, Accuracy: 0.6420\n",
            "Batch 171: Loss: 1.2267, Accuracy: 0.7232\n",
            "Batch 181: Loss: 1.1901, Accuracy: 0.7907\n",
            "Batch 191: Loss: 1.2086, Accuracy: 0.7238\n",
            "Batch 201: Loss: 1.2378, Accuracy: 0.6957\n",
            "Batch 211: Loss: 1.1899, Accuracy: 0.7308\n",
            "Batch 221: Loss: 1.2028, Accuracy: 0.6530\n",
            "Batch 231: Loss: 1.2251, Accuracy: 0.6870\n",
            "Batch 241: Loss: 1.1897, Accuracy: 0.8177\n",
            "Batch 251: Loss: 1.2114, Accuracy: 0.6370\n",
            "Batch 261: Loss: 1.1948, Accuracy: 0.7980\n",
            "Batch 271: Loss: 1.2034, Accuracy: 0.7642\n",
            "Batch 281: Loss: 1.2022, Accuracy: 0.7113\n",
            "Batch 291: Loss: 1.1816, Accuracy: 0.7113\n",
            "Batch 301: Loss: 1.1988, Accuracy: 0.6525\n",
            "Batch 311: Loss: 1.2152, Accuracy: 0.6620\n",
            "Batch 321: Loss: 1.2021, Accuracy: 0.7040\n",
            "Batch 331: Loss: 1.2095, Accuracy: 0.7220\n",
            "Batch 341: Loss: 1.1750, Accuracy: 0.7880\n",
            "Batch 351: Loss: 1.1883, Accuracy: 0.6985\n",
            "Batch 361: Loss: 1.2168, Accuracy: 0.6850\n",
            "Batch 371: Loss: 1.1856, Accuracy: 0.6575\n",
            "Batch 381: Loss: 1.2161, Accuracy: 0.7762\n",
            "Batch 391: Loss: 1.2303, Accuracy: 0.5877\n",
            "Batch 401: Loss: 1.1999, Accuracy: 0.5960\n",
            "Batch 411: Loss: 1.1745, Accuracy: 0.7505\n",
            "Batch 421: Loss: 1.1960, Accuracy: 0.6705\n",
            "Batch 431: Loss: 1.1978, Accuracy: 0.7165\n",
            "Batch 441: Loss: 1.2082, Accuracy: 0.8283\n",
            "Batch 451: Loss: 1.1961, Accuracy: 0.6332\n",
            "Batch 461: Loss: 1.2505, Accuracy: 0.5132\n",
            "Batch 471: Loss: 1.1937, Accuracy: 0.8442\n",
            "Batch 481: Loss: 1.2195, Accuracy: 0.6663\n",
            "Batch 491: Loss: 1.1923, Accuracy: 0.5563\n",
            "Batch 501: Loss: 1.2360, Accuracy: 0.6332\n",
            "Batch 511: Loss: 1.2030, Accuracy: 0.6110\n",
            "Batch 521: Loss: 1.1990, Accuracy: 0.7415\n",
            "Batch 531: Loss: 1.2014, Accuracy: 0.7003\n",
            "Batch 541: Loss: 1.1868, Accuracy: 0.6795\n",
            "Batch 551: Loss: 1.2325, Accuracy: 0.6462\n",
            "Batch 561: Loss: 1.1924, Accuracy: 0.7887\n",
            "Batch 571: Loss: 1.1994, Accuracy: 0.7212\n",
            "Batch 581: Loss: 1.1810, Accuracy: 0.7933\n",
            "Batch 591: Loss: 1.1901, Accuracy: 0.6090\n",
            "Batch 601: Loss: 1.1785, Accuracy: 0.7167\n",
            "Batch 611: Loss: 1.2197, Accuracy: 0.8043\n",
            "Batch 621: Loss: 1.1947, Accuracy: 0.7053\n",
            "Batch 631: Loss: 1.2232, Accuracy: 0.6018\n",
            "Batch 641: Loss: 1.1810, Accuracy: 0.7823\n",
            "Batch 651: Loss: 1.2094, Accuracy: 0.7795\n",
            "Batch 661: Loss: 1.2060, Accuracy: 0.7445\n",
            "Batch 671: Loss: 1.1942, Accuracy: 0.6647\n",
            "Batch 681: Loss: 1.1780, Accuracy: 0.7572\n",
            "Batch 691: Loss: 1.2005, Accuracy: 0.7160\n",
            "Batch 701: Loss: 1.1733, Accuracy: 0.7475\n",
            "Batch 711: Loss: 1.2084, Accuracy: 0.7670\n",
            "Batch 721: Loss: 1.2569, Accuracy: 0.6705\n",
            "Batch 731: Loss: 1.2049, Accuracy: 0.7515\n",
            "Batch 741: Loss: 1.1964, Accuracy: 0.8215\n",
            "Batch 751: Loss: 1.1808, Accuracy: 0.8175\n",
            "Batch 761: Loss: 1.2054, Accuracy: 0.6575\n",
            "Batch 771: Loss: 1.1805, Accuracy: 0.7740\n",
            "Batch 781: Loss: 1.2158, Accuracy: 0.7768\n",
            "Batch 791: Loss: 1.1894, Accuracy: 0.7085\n",
            "Batch 801: Loss: 1.1892, Accuracy: 0.7642\n",
            "Batch 811: Loss: 1.1769, Accuracy: 0.7755\n",
            "Batch 821: Loss: 1.1918, Accuracy: 0.7127\n",
            "Batch 831: Loss: 1.2157, Accuracy: 0.5845\n",
            "Batch 841: Loss: 1.1912, Accuracy: 0.6860\n",
            "Batch 851: Loss: 1.1688, Accuracy: 0.8283\n",
            "Batch 861: Loss: 1.2248, Accuracy: 0.7712\n",
            "Batch 871: Loss: 1.2089, Accuracy: 0.7943\n",
            "Batch 881: Loss: 1.1659, Accuracy: 0.8280\n",
            "Batch 891: Loss: 1.2191, Accuracy: 0.7762\n",
            "Batch 901: Loss: 1.2003, Accuracy: 0.7678\n",
            "Batch 911: Loss: 1.1871, Accuracy: 0.7410\n",
            "Batch 921: Loss: 1.1974, Accuracy: 0.6660\n",
            "Batch 931: Loss: 1.1754, Accuracy: 0.7522\n",
            "Batch 941: Loss: 1.1969, Accuracy: 0.6673\n",
            "Batch 951: Loss: 1.2227, Accuracy: 0.6725\n",
            "Batch 961: Loss: 1.1895, Accuracy: 0.7117\n",
            "Batch 971: Loss: 1.2129, Accuracy: 0.7147\n",
            "Batch 981: Loss: 1.1878, Accuracy: 0.7515\n",
            "Batch 991: Loss: 1.1713, Accuracy: 0.7500\n",
            "Batch 1001: Loss: 1.2240, Accuracy: 0.7033\n",
            "Batch 1011: Loss: 1.1956, Accuracy: 0.6853\n",
            "Batch 1021: Loss: 1.2119, Accuracy: 0.7883\n",
            "Batch 1031: Loss: 1.2321, Accuracy: 0.7095\n",
            "Batch 1041: Loss: 1.2257, Accuracy: 0.6430\n",
            "Batch 1051: Loss: 1.2022, Accuracy: 0.6745\n",
            "Batch 1061: Loss: 1.2313, Accuracy: 0.6342\n",
            "Batch 1071: Loss: 1.1959, Accuracy: 0.7000\n",
            "Batch 1081: Loss: 1.2095, Accuracy: 0.6258\n",
            "Batch 1091: Loss: 1.1948, Accuracy: 0.8828\n",
            "Batch 1101: Loss: 1.2009, Accuracy: 0.6330\n",
            "Batch 1111: Loss: 1.2283, Accuracy: 0.6867\n",
            "Batch 1121: Loss: 1.2310, Accuracy: 0.5305\n",
            "Batch 1131: Loss: 1.1716, Accuracy: 0.8430\n",
            "Batch 1141: Loss: 1.1965, Accuracy: 0.7378\n",
            "Batch 1151: Loss: 1.2110, Accuracy: 0.7163\n",
            "Batch 1161: Loss: 1.1976, Accuracy: 0.7063\n",
            "Batch 1171: Loss: 1.1953, Accuracy: 0.6310\n",
            "Batch 1181: Loss: 1.2003, Accuracy: 0.6680\n",
            "Batch 1191: Loss: 1.2344, Accuracy: 0.4562\n",
            "Batch 1201: Loss: 1.2130, Accuracy: 0.7003\n",
            "Batch 1211: Loss: 1.2066, Accuracy: 0.7192\n",
            "Batch 1221: Loss: 1.2077, Accuracy: 0.7260\n",
            "Batch 1231: Loss: 1.2007, Accuracy: 0.6472\n",
            "Batch 1241: Loss: 1.1924, Accuracy: 0.6178\n",
            "Batch 1251: Loss: 1.2066, Accuracy: 0.7202\n",
            "Batch 1261: Loss: 1.1883, Accuracy: 0.7530\n",
            "Batch 1271: Loss: 1.2046, Accuracy: 0.8100\n",
            "Batch 1281: Loss: 1.1955, Accuracy: 0.6955\n",
            "Batch 1291: Loss: 1.1967, Accuracy: 0.6715\n",
            "Batch 1301: Loss: 1.1990, Accuracy: 0.7800\n",
            "Batch 1311: Loss: 1.2103, Accuracy: 0.8263\n",
            "Finished evaluation - Average Loss: 1.2028, Accuracy: 0.7093\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assume model, validation_dataloader, criterion, and device are already defined\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m average_loss_val, accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, criterion, device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation results - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# Assume model, validation_dataloader, criterion, and device are already defined\n",
        "average_loss_val, accuracy = evaluate_model(model, val_loader, criterion, device)\n",
        "print(f\"Validation results - Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_rPyJs0nME3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history_val, label='Loss per Batch')\n",
        "plt.title('Evaluation Loss History')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ZBHUOSnME3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Project1",
      "language": "python",
      "name": "testname"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}