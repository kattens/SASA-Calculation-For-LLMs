{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/SASA-Calculation-For-LLMs/blob/main/Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_lNWkydOc7g",
        "outputId": "d7341502-7533-49fc-a6cf-68c8fefcfb3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1468"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Clear the CUDA memory cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Optionally, collect garbage to free up memory from unused objects\n",
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkG-WemRazLx"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36P5XPm9bLhe"
      },
      "source": [
        "# Agenda for the code:\n",
        "just give the global seq as input and local as label to the model without adding new tokens just do a prediction task simple as this then build up\n",
        "\n",
        "\n",
        "\n",
        "#outline of the code:\n",
        "import the dataset\n",
        "\n",
        "import tokenizer / base model\n",
        "\n",
        "build dataset class\n",
        "\n",
        "make the dataloader\n",
        "\n",
        "split the test and training data\n",
        "\n",
        "train\n",
        "\n",
        "test\n",
        "\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWSbp4XSbBi4"
      },
      "outputs": [],
      "source": [
        "pairs_df = pd.read_csv('/home/k_ensafitakaldani001_umb_edu/Project1/merged1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAvCVYi0b9O-"
      },
      "source": [
        "dataset:\n",
        "first -> make 1 input from seq A and B\n",
        "sencond -> make masked seq A and B as their labels\n",
        "\n",
        "goal: make a model that can predict the local sequences of A and B based on global sequences A and B.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "inputs:\n",
        "[Entity 1] abweregfvkk [Entity 2] qwmfdefjlxcvg\n",
        "\n",
        "predictions: predictions should be something like:\n",
        "[Entity 1] ab---egf--- [Entity 2] ---fde---xcvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uXfKGHYnMEx",
        "outputId": "21a5a249-d368-41c2-c37b-518daced0908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59960\n"
          ]
        }
      ],
      "source": [
        "df = pairs_df\n",
        "filtered_df = df[(df['Sequence_A'].str.len() >= 50) & (df['Sequence_A'].str.len() < 200)]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU_Rpwmj1AcB",
        "outputId": "0d5985fe-a05a-4c3e-d944-4ff31854abde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token '[ENTITY1]' has ID: 30\n",
            "Token '[ENTITY2]' has ID: 31\n",
            "Token '-' has ID: 32\n",
            "Token 'BR' has ID: 33\n",
            "Token 'PE' has ID: 34\n",
            "Token 'EX' has ID: 35\n",
            "Token embeddings resized to accommodate new tokens.\n",
            "Updated vocabulary size: 36\n",
            "All special tokens are in the tokenizer's vocabulary.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import BertModel, BertConfig, AutoTokenizer\n",
        "\n",
        "# Initialize the ProtBERT tokenizer and model -> mainly use these for pretraining\n",
        "\n",
        "# used for a variety of downstream tasks (e.g., classification, tagging). Unlike AutoModelForMaskedLM, it is not specifically tied to masked language modeling\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "\n",
        "# Define special tokens for entities\n",
        "special_tokens = ['[ENTITY1]', '[ENTITY2]', '-', 'BR', 'PE', 'EX']\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Check if the special tokens were added successfully\n",
        "print(f\"Token '[ENTITY1]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY1]')}\")\n",
        "print(f\"Token '[ENTITY2]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY2]')}\")\n",
        "print(f\"Token '-' has ID: {tokenizer.convert_tokens_to_ids('-')}\")\n",
        "print(f\"Token 'BR' has ID: {tokenizer.convert_tokens_to_ids('BR')}\")\n",
        "print(f\"Token 'PE' has ID: {tokenizer.convert_tokens_to_ids('PE')}\")\n",
        "print(f\"Token 'EX' has ID: {tokenizer.convert_tokens_to_ids('EX')}\")\n",
        "\n",
        "# Resize the model's embedding size to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print('Token embeddings resized to accommodate new tokens.')\n",
        "\n",
        "# Helper function to convert numerical token IDs back to their textual representation\n",
        "def ids_to_text(ids):\n",
        "    return ' '.join(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "# Check the updated size of the tokenizer's vocabulary\n",
        "print(f\"Updated vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "# Check if the new tokens are in the tokenizer's vocabulary\n",
        "if all(token in tokenizer.get_vocab() for token in special_tokens):\n",
        "    print(\"All special tokens are in the tokenizer's vocabulary.\")\n",
        "else:\n",
        "    print(\"Some special tokens are NOT in the tokenizer's vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uic0UPfg6nhS"
      },
      "outputs": [],
      "source": [
        "vocab= tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0uacbmd6sAi",
        "outputId": "a801b504-29e2-4001-f3a2-e1f27e0ba9e9",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEqPZlLZvKGV",
        "outputId": "a4c63f89-5442-4e9f-8aa3-2b337eeb400d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        }
      ],
      "source": [
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(tokenizer.get_vocab())  # Adjust this if your task isn't directly classification\n",
        "print(num_amino_acids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEgjaqlqO7IC"
      },
      "outputs": [],
      "source": [
        "#This is a vertsion which we dont need now\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_SASA_sequence(self, row):\n",
        "        SASA_A = row['SASA_Category_A']\n",
        "        SASA_B = row['SASA_Category_B']\n",
        "\n",
        "        try:\n",
        "            if isinstance(SASA_A, str):\n",
        "                SASA_A = SASA_A.split(\", \")\n",
        "            elif not isinstance(SASA_A, list):\n",
        "                SASA_A = [str(SASA_A)]\n",
        "\n",
        "            if isinstance(SASA_B, str):\n",
        "                SASA_B = SASA_B.split(\", \")\n",
        "            elif not isinstance(SASA_B, list):\n",
        "                SASA_B = [str(SASA_B)]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing SASA sequences: {e}\")\n",
        "            print(f\"SASA_A: {SASA_A}, SASA_B: {SASA_B}\")\n",
        "\n",
        "        SASA_sequence = f\"[ENTITY1] {' '.join(SASA_A)} [SEP] [ENTITY2] {' '.join(SASA_B)}\"\n",
        "        return SASA_sequence\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx]\n",
        "        Global_Sequence_A = row['Sequence_A']\n",
        "        Global_Sequence_B = row['Sequence_B']\n",
        "        Local_Sequence_A = row['masked_sequence_A']\n",
        "        Local_Sequence_B = row['masked_sequence_B']\n",
        "\n",
        "        # Enhanced sequences with new tokens\n",
        "        Global_sequence = f\"[ENTITY1] {Global_Sequence_A} [SEP] [ENTITY2] {Global_Sequence_B}\"\n",
        "        Local_sequence = f\"[ENTITY1] {Local_Sequence_A} [SEP] [ENTITY2] {Local_Sequence_B}\"\n",
        "        SASA_sequence = self.get_SASA_sequence(row)\n",
        "\n",
        "        # Tokenize input, label, and SASA sequences\n",
        "        global_inputs = self.tokenizer(Global_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        labels = self.tokenizer(Local_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        SASA_inputs = self.tokenizer(SASA_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "\n",
        "        # Ensure all outputs are properly returned\n",
        "        return {\n",
        "            'global_input_ids': global_inputs['input_ids'].squeeze(0),\n",
        "            'global_attention_mask': global_inputs['attention_mask'].squeeze(0),\n",
        "            'SASA_input_ids': SASA_inputs['input_ids'].squeeze(0),  # SASA tokenized and part of input\n",
        "            'SASA_attention_mask': SASA_inputs['attention_mask'].squeeze(0),  # Attention mask for SASA inputs\n",
        "            'labels': labels['input_ids'].squeeze(0)  # Local sequences as labels\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs = {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fanKkNHFdzHS",
        "outputId": "f0620506-e208-48c7-f20b-dca20a7b3c3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#BASE CODE\\nimport torch\\nimport torch.nn as nn\\n\\nclass ProtBertSeq2Seq(nn.Module):\\n    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\\n        super(ProtBertSeq2Seq, self).__init__()\\n        self.model = model  # The underlying ProtBERT model\\n        self.seq_len = seq_len  # Sequence length\\n        self.num_amino_acids = num_amino_acids  # Total number of possible amino acids\\n        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer to prevent overfitting\\n        # Separate classifiers for each input might be considered depending on the model design\\n        self.classifier = nn.Linear(model.config.hidden_size * 2, num_amino_acids)  # Classifier to predict amino acids based on concatenated outputs\\n\\n    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\\n        # Process global sequence inputs\\n        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\\n        global_outputs = self.dropout(global_outputs)\\n\\n        # Process SASA sequence inputs\\n        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\\n        SASA_outputs = self.dropout(SASA_outputs)\\n\\n        # Combine the outputs from the two channels\\n        # This simple concatenation can be replaced with more sophisticated methods (e.g., addition, averaging)\\n        combined_outputs = torch.cat((global_outputs, SASA_outputs), dim=-1)\\n\\n        # Pass through the classifier to get predictions\\n        logits = self.classifier(combined_outputs)\\n        return logits\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#BASE CODE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model  # The underlying ProtBERT model\n",
        "        self.seq_len = seq_len  # Sequence length\n",
        "        self.num_amino_acids = num_amino_acids  # Total number of possible amino acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer to prevent overfitting\n",
        "        # Separate classifiers for each input might be considered depending on the model design\n",
        "        self.classifier = nn.Linear(model.config.hidden_size * 2, num_amino_acids)  # Classifier to predict amino acids based on concatenated outputs\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Combine the outputs from the two channels\n",
        "        # This simple concatenation can be replaced with more sophisticated methods (e.g., addition, averaging)\n",
        "        combined_outputs = torch.cat((global_outputs, SASA_outputs), dim=-1)\n",
        "\n",
        "        # Pass through the classifier to get predictions\n",
        "        logits = self.classifier(combined_outputs)\n",
        "        return logits\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RlDqXC3LOc8C",
        "outputId": "6bd160b3-a870-4442-c4ec-5878c25b4a47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#Option 1: Element-wise Multiplication (Hadamard Product)\\nclass ProtBertSeq2Seq(nn.Module):\\n    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\\n        super(ProtBertSeq2Seq, self).__init__()\\n        self.model = model\\n        self.seq_len = seq_len\\n        self.num_amino_acids = num_amino_acids\\n        self.dropout = nn.Dropout(dropout_rate)\\n\\n        # Classifier for prediction\\n        self.classifier = nn.Linear(model.config.hidden_size, num_amino_acids)\\n\\n    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\\n        # Process global sequence inputs\\n        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\\n        global_outputs = self.dropout(global_outputs)\\n\\n        # Process SASA sequence inputs\\n        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\\n        SASA_outputs = self.dropout(SASA_outputs)\\n\\n        # Apply element-wise multiplication (Hadamard product)\\n        combined_outputs = global_outputs * SASA_outputs\\n\\n        # Pass through classifier\\n        logits = self.classifier(combined_outputs)\\n        return logits\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#Option 1: Element-wise Multiplication (Hadamard Product)\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_amino_acids = num_amino_acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Classifier for prediction\n",
        "        self.classifier = nn.Linear(model.config.hidden_size, num_amino_acids)\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Apply element-wise multiplication (Hadamard product)\n",
        "        combined_outputs = global_outputs * SASA_outputs\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(combined_outputs)\n",
        "        return logits\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-oW0BA9dOc8D",
        "outputId": "c8133816-b554-4cb8-97fb-66a0394f5f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#Gated Mechanism\\nclass ProtBertSeq2Seq(nn.Module):\\n    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\\n        super(ProtBertSeq2Seq, self).__init__()\\n        self.model = model\\n        self.seq_len = seq_len\\n        self.num_amino_acids = num_amino_acids\\n        self.dropout = nn.Dropout(dropout_rate)\\n\\n        # Gating mechanism to learn weights between global and SASA outputs\\n        self.gate = nn.Linear(model.config.hidden_size * 2, model.config.hidden_size)\\n\\n        # Classifier for prediction\\n        self.classifier = nn.Linear(model.config.hidden_size, num_amino_acids)\\n\\n    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\\n        # Process global sequence inputs\\n        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\\n        global_outputs = self.dropout(global_outputs)\\n\\n        # Process SASA sequence inputs\\n        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\\n        SASA_outputs = self.dropout(SASA_outputs)\\n\\n        # Concatenate and apply a gating mechanism to mix the two inputs element-wise\\n        combined_inputs = torch.cat((global_outputs, SASA_outputs), dim=-1)\\n        mixed_outputs = torch.sigmoid(self.gate(combined_inputs)) * global_outputs + (1 - torch.sigmoid(self.gate(combined_inputs))) * SASA_outputs\\n\\n        # Pass through classifier\\n        logits = self.classifier(mixed_outputs)\\n        return logits\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#Gated Mechanism\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_amino_acids = num_amino_acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Gating mechanism to learn weights between global and SASA outputs\n",
        "        self.gate = nn.Linear(model.config.hidden_size * 2, model.config.hidden_size)\n",
        "\n",
        "        # Classifier for prediction\n",
        "        self.classifier = nn.Linear(model.config.hidden_size, num_amino_acids)\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Concatenate and apply a gating mechanism to mix the two inputs element-wise\n",
        "        combined_inputs = torch.cat((global_outputs, SASA_outputs), dim=-1)\n",
        "        mixed_outputs = torch.sigmoid(self.gate(combined_inputs)) * global_outputs + (1 - torch.sigmoid(self.gate(combined_inputs))) * SASA_outputs\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(mixed_outputs)\n",
        "        return logits\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YuhphgpaOc8E"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Transformer Encoder Layer\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_amino_acids = num_amino_acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Attention layer to mix global and SASA features\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=model.config.hidden_size, num_heads=8)\n",
        "\n",
        "        # Classifier for prediction\n",
        "        self.classifier = nn.Linear(model.config.hidden_size, num_amino_acids)\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Apply cross-attention where global attends to SASA\n",
        "        attn_output, _ = self.attention(global_outputs, SASA_outputs, SASA_outputs)\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(attn_output)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkSx2nzduaYb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def save_checkpoint(state, filename=\"/home/k_ensafitakaldani001_umb_edu/Project1/checkpoint240.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epochs, accumulation_steps=2, checkpoint_path=\"checkpoint240.pth.tar\"):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    start_epoch = 0\n",
        "    loss_history = []\n",
        "    scaler = GradScaler()  # Mixed precision scaler\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss_history = checkpoint['loss_history']\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():  # Mixed precision forward pass\n",
        "                outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "                loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1)) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()  # Scaled backward pass\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:  # Perform optimizer step every few batches\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps  # Accumulate the loss\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}')\n",
        "\n",
        "            # Freeing memory after each batch\n",
        "            del global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask, labels, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        average_loss = total_loss / len(dataloader)\n",
        "        loss_history.append(average_loss)\n",
        "        print(f'End of Epoch {epoch + 1}, Average Loss: {average_loss}')\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss_history': loss_history\n",
        "        }, filename=checkpoint_path)\n",
        "\n",
        "    return loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_kGPlLX8Oai",
        "outputId": "389d084c-5a0c-45aa-fb55-83ed45538369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset size: 59960\n",
            "Training data size: 47968\n",
            "Number of batches in train_loader: 11992, Each batch has 4 samples.\n",
            "Number of batches in val_loader: 2998, Each batch has 4 samples.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#split the train and test here\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "train_df, val_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "\n",
        "# Setup DataLoaders for training and validation\n",
        "train_dataset = SampleDataset(train_df, tokenizer,450)\n",
        "val_dataset = SampleDataset(val_df, tokenizer, 450)\n",
        "\n",
        "\n",
        "# Using pin_memory for faster host to device transfer\n",
        "# Increasing num_workers to use multiple CPU cores for data loading\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn,  num_workers=2, pin_memory=True)\n",
        "\n",
        "# Log the setup\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}, Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}, Each batch has {val_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J21E7fYePtRK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Im9Ou7YeAf",
        "outputId": "7efdbe4b-8b47-46d7-8f16-bc14acb0b1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 425155620\n",
            "Trainable parameters: 983040\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from transformers import AutoTokenizer, BertModel  # Ensure you are using BertModel\n",
        "from peft import LoraModel, LoraConfig\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = tokenizer\n",
        "\n",
        "# Load the base BertModel or similar from transformers suited for your needs\n",
        "base_model = model  # Replace with your base model\n",
        "\n",
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(vocab)  # Adjust this if your task isn't directly classification\n",
        "\n",
        "# Initialize your custom model with the base model\n",
        "my_model = ProtBertSeq2Seq(model=base_model, num_amino_acids=num_amino_acids, seq_len=500)\n",
        "\n",
        "# LoRA configuration\n",
        "config = LoraConfig(\n",
        "    task_type=\"SEQ_2_SEQ_LM\",  # Confirm this is your intended task\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"value\", \"key\"],  # Make sure these modules exist in your base model\n",
        "    lora_dropout=0.01\n",
        ")\n",
        "\n",
        "# Initialize LoRA model with the custom model and specify the adapter name if necessary\n",
        "lora_model = LoraModel(my_model, config, adapter_name=\"default\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lora_model.to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = Adam(lora_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Check parameters and setup\n",
        "print(f\"Total parameters: {sum(p.numel() for p in lora_model.parameters())}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZPJ8NNdYhWX",
        "tags": [],
        "outputId": "49020205-7900-493d-cdc7-34fa087d879e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1157671/1435818670.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Mixed precision scaler\n",
            "/tmp/ipykernel_1157671/1435818670.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Mixed precision forward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 1.7951685190200806\n",
            "Epoch 1, Batch 11, Loss: 1.6856964826583862\n",
            "Epoch 1, Batch 21, Loss: 1.5770866870880127\n",
            "Epoch 1, Batch 31, Loss: 1.5171517133712769\n",
            "Epoch 1, Batch 41, Loss: 1.4789113998413086\n",
            "Epoch 1, Batch 51, Loss: 1.4645969867706299\n",
            "Epoch 1, Batch 61, Loss: 1.4494715929031372\n",
            "Epoch 1, Batch 71, Loss: 1.427311897277832\n",
            "Epoch 1, Batch 81, Loss: 1.4206254482269287\n",
            "Epoch 1, Batch 91, Loss: 1.4181959629058838\n",
            "Epoch 1, Batch 101, Loss: 1.4227018356323242\n",
            "Epoch 1, Batch 111, Loss: 1.4049453735351562\n",
            "Epoch 1, Batch 121, Loss: 1.3932123184204102\n",
            "Epoch 1, Batch 131, Loss: 1.3847213983535767\n",
            "Epoch 1, Batch 141, Loss: 1.382173776626587\n",
            "Epoch 1, Batch 151, Loss: 1.378298044204712\n",
            "Epoch 1, Batch 161, Loss: 1.3815128803253174\n",
            "Epoch 1, Batch 171, Loss: 1.3645168542861938\n",
            "Epoch 1, Batch 181, Loss: 1.3843140602111816\n",
            "Epoch 1, Batch 191, Loss: 1.3688852787017822\n",
            "Epoch 1, Batch 201, Loss: 1.3743925094604492\n",
            "Epoch 1, Batch 211, Loss: 1.3661781549453735\n",
            "Epoch 1, Batch 221, Loss: 1.361480474472046\n",
            "Epoch 1, Batch 231, Loss: 1.364517331123352\n",
            "Epoch 1, Batch 241, Loss: 1.3601205348968506\n",
            "Epoch 1, Batch 251, Loss: 1.3562140464782715\n",
            "Epoch 1, Batch 261, Loss: 1.3698493242263794\n",
            "Epoch 1, Batch 271, Loss: 1.3523764610290527\n",
            "Epoch 1, Batch 281, Loss: 1.351554274559021\n",
            "Epoch 1, Batch 291, Loss: 1.3422049283981323\n",
            "Epoch 1, Batch 301, Loss: 1.3590925931930542\n",
            "Epoch 1, Batch 311, Loss: 1.3484971523284912\n",
            "Epoch 1, Batch 321, Loss: 1.3452324867248535\n",
            "Epoch 1, Batch 331, Loss: 1.353684425354004\n",
            "Epoch 1, Batch 341, Loss: 1.3416192531585693\n",
            "Epoch 1, Batch 351, Loss: 1.3446364402770996\n",
            "Epoch 1, Batch 361, Loss: 1.3558619022369385\n",
            "Epoch 1, Batch 371, Loss: 1.350037932395935\n",
            "Epoch 1, Batch 381, Loss: 1.3382315635681152\n",
            "Epoch 1, Batch 391, Loss: 1.35285222530365\n",
            "Epoch 1, Batch 401, Loss: 1.3564707040786743\n",
            "Epoch 1, Batch 411, Loss: 1.3575236797332764\n",
            "Epoch 1, Batch 421, Loss: 1.334511399269104\n",
            "Epoch 1, Batch 431, Loss: 1.3443080186843872\n",
            "Epoch 1, Batch 441, Loss: 1.3441338539123535\n",
            "Epoch 1, Batch 451, Loss: 1.3458640575408936\n",
            "Epoch 1, Batch 461, Loss: 1.3342188596725464\n",
            "Epoch 1, Batch 471, Loss: 1.3506029844284058\n",
            "Epoch 1, Batch 481, Loss: 1.3477697372436523\n",
            "Epoch 1, Batch 491, Loss: 1.3462564945220947\n",
            "Epoch 1, Batch 501, Loss: 1.3415851593017578\n",
            "Epoch 1, Batch 511, Loss: 1.328773021697998\n",
            "Epoch 1, Batch 521, Loss: 1.3286519050598145\n",
            "Epoch 1, Batch 531, Loss: 1.3332695960998535\n",
            "Epoch 1, Batch 541, Loss: 1.3473293781280518\n",
            "Epoch 1, Batch 551, Loss: 1.3407313823699951\n",
            "Epoch 1, Batch 561, Loss: 1.3318564891815186\n",
            "Epoch 1, Batch 571, Loss: 1.3291661739349365\n",
            "Epoch 1, Batch 581, Loss: 1.339043140411377\n",
            "Epoch 1, Batch 591, Loss: 1.3306756019592285\n",
            "Epoch 1, Batch 601, Loss: 1.325290322303772\n",
            "Epoch 1, Batch 611, Loss: 1.3294481039047241\n",
            "Epoch 1, Batch 621, Loss: 1.3251677751541138\n",
            "Epoch 1, Batch 631, Loss: 1.3313144445419312\n",
            "Epoch 1, Batch 641, Loss: 1.3270505666732788\n",
            "Epoch 1, Batch 651, Loss: 1.3271530866622925\n",
            "Epoch 1, Batch 661, Loss: 1.3280975818634033\n",
            "Epoch 1, Batch 671, Loss: 1.3260949850082397\n",
            "Epoch 1, Batch 681, Loss: 1.3308757543563843\n",
            "Epoch 1, Batch 691, Loss: 1.3316011428833008\n",
            "Epoch 1, Batch 701, Loss: 1.343890905380249\n",
            "Epoch 1, Batch 711, Loss: 1.3342119455337524\n",
            "Epoch 1, Batch 721, Loss: 1.340267300605774\n",
            "Epoch 1, Batch 731, Loss: 1.328540325164795\n",
            "Epoch 1, Batch 741, Loss: 1.3370281457901\n",
            "Epoch 1, Batch 751, Loss: 1.3353883028030396\n",
            "Epoch 1, Batch 761, Loss: 1.3275771141052246\n",
            "Epoch 1, Batch 771, Loss: 1.335824728012085\n",
            "Epoch 1, Batch 781, Loss: 1.3292038440704346\n",
            "Epoch 1, Batch 791, Loss: 1.354453444480896\n",
            "Epoch 1, Batch 801, Loss: 1.334031343460083\n",
            "Epoch 1, Batch 811, Loss: 1.317668080329895\n",
            "Epoch 1, Batch 821, Loss: 1.3280034065246582\n",
            "Epoch 1, Batch 831, Loss: 1.3236989974975586\n",
            "Epoch 1, Batch 841, Loss: 1.3353707790374756\n",
            "Epoch 1, Batch 851, Loss: 1.3276256322860718\n",
            "Epoch 1, Batch 861, Loss: 1.3476260900497437\n",
            "Epoch 1, Batch 871, Loss: 1.3250423669815063\n",
            "Epoch 1, Batch 881, Loss: 1.337448000907898\n",
            "Epoch 1, Batch 891, Loss: 1.329164743423462\n",
            "Epoch 1, Batch 901, Loss: 1.326690673828125\n",
            "Epoch 1, Batch 911, Loss: 1.323778748512268\n",
            "Epoch 1, Batch 921, Loss: 1.3292224407196045\n",
            "Epoch 1, Batch 931, Loss: 1.3331458568572998\n",
            "Epoch 1, Batch 941, Loss: 1.329473614692688\n",
            "Epoch 1, Batch 951, Loss: 1.329131841659546\n",
            "Epoch 1, Batch 961, Loss: 1.319206714630127\n",
            "Epoch 1, Batch 971, Loss: 1.3242918252944946\n",
            "Epoch 1, Batch 981, Loss: 1.3228954076766968\n",
            "Epoch 1, Batch 991, Loss: 1.3277068138122559\n",
            "Epoch 1, Batch 1001, Loss: 1.324523687362671\n",
            "Epoch 1, Batch 1011, Loss: 1.3241480588912964\n",
            "Epoch 1, Batch 1021, Loss: 1.3258060216903687\n",
            "Epoch 1, Batch 1031, Loss: 1.3270248174667358\n",
            "Epoch 1, Batch 1041, Loss: 1.3238725662231445\n",
            "Epoch 1, Batch 1051, Loss: 1.3280386924743652\n",
            "Epoch 1, Batch 1061, Loss: 1.3184014558792114\n",
            "Epoch 1, Batch 1071, Loss: 1.3277461528778076\n",
            "Epoch 1, Batch 1081, Loss: 1.3185558319091797\n",
            "Epoch 1, Batch 1091, Loss: 1.3168156147003174\n",
            "Epoch 1, Batch 1101, Loss: 1.3251571655273438\n",
            "Epoch 1, Batch 1111, Loss: 1.3169035911560059\n",
            "Epoch 1, Batch 1121, Loss: 1.3300445079803467\n",
            "Epoch 1, Batch 1131, Loss: 1.3385870456695557\n",
            "Epoch 1, Batch 1141, Loss: 1.3326610326766968\n",
            "Epoch 1, Batch 1151, Loss: 1.3416661024093628\n",
            "Epoch 1, Batch 1161, Loss: 1.3166767358779907\n",
            "Epoch 1, Batch 1171, Loss: 1.3648521900177002\n",
            "Epoch 1, Batch 1181, Loss: 1.3305763006210327\n",
            "Epoch 1, Batch 1191, Loss: 1.3407485485076904\n",
            "Epoch 1, Batch 1201, Loss: 1.3228272199630737\n",
            "Epoch 1, Batch 1211, Loss: 1.3225990533828735\n",
            "Epoch 1, Batch 1221, Loss: 1.3231348991394043\n",
            "Epoch 1, Batch 1231, Loss: 1.3297756910324097\n",
            "Epoch 1, Batch 1241, Loss: 1.3340911865234375\n",
            "Epoch 1, Batch 1251, Loss: 1.3154218196868896\n",
            "Epoch 1, Batch 1261, Loss: 1.323169231414795\n",
            "Epoch 1, Batch 1271, Loss: 1.3182872533798218\n",
            "Epoch 1, Batch 1281, Loss: 1.3443655967712402\n",
            "Epoch 1, Batch 1291, Loss: 1.3251826763153076\n",
            "Epoch 1, Batch 1301, Loss: 1.3224207162857056\n",
            "Epoch 1, Batch 1311, Loss: 1.3259880542755127\n",
            "Epoch 1, Batch 1321, Loss: 1.3218469619750977\n",
            "Epoch 1, Batch 1331, Loss: 1.327404260635376\n",
            "Epoch 1, Batch 1341, Loss: 1.316114902496338\n",
            "Epoch 1, Batch 1351, Loss: 1.3216675519943237\n",
            "Epoch 1, Batch 1361, Loss: 1.328047513961792\n",
            "Epoch 1, Batch 1371, Loss: 1.3306903839111328\n",
            "Epoch 1, Batch 1381, Loss: 1.342667818069458\n",
            "Epoch 1, Batch 1391, Loss: 1.3359119892120361\n",
            "Epoch 1, Batch 1401, Loss: 1.3162856101989746\n",
            "Epoch 1, Batch 1411, Loss: 1.329765796661377\n",
            "Epoch 1, Batch 1421, Loss: 1.3243017196655273\n",
            "Epoch 1, Batch 1431, Loss: 1.317338228225708\n",
            "Epoch 1, Batch 1441, Loss: 1.317013144493103\n",
            "Epoch 1, Batch 1451, Loss: 1.328192949295044\n",
            "Epoch 1, Batch 1461, Loss: 1.327679991722107\n",
            "Epoch 1, Batch 1471, Loss: 1.3261322975158691\n",
            "Epoch 1, Batch 1481, Loss: 1.3232790231704712\n",
            "Epoch 1, Batch 1491, Loss: 1.3154796361923218\n",
            "Epoch 1, Batch 1501, Loss: 1.3245223760604858\n",
            "Epoch 1, Batch 1511, Loss: 1.318790078163147\n",
            "Epoch 1, Batch 1521, Loss: 1.3317290544509888\n",
            "Epoch 1, Batch 1531, Loss: 1.3142011165618896\n",
            "Epoch 1, Batch 1541, Loss: 1.315782070159912\n",
            "Epoch 1, Batch 1551, Loss: 1.3286253213882446\n",
            "Epoch 1, Batch 1561, Loss: 1.315395474433899\n",
            "Epoch 1, Batch 1571, Loss: 1.3474905490875244\n",
            "Epoch 1, Batch 1581, Loss: 1.3277807235717773\n",
            "Epoch 1, Batch 1591, Loss: 1.3375897407531738\n",
            "Epoch 1, Batch 1601, Loss: 1.3213053941726685\n",
            "Epoch 1, Batch 1611, Loss: 1.3211100101470947\n",
            "Epoch 1, Batch 1621, Loss: 1.3344807624816895\n",
            "Epoch 1, Batch 1631, Loss: 1.3253180980682373\n",
            "Epoch 1, Batch 1641, Loss: 1.322617530822754\n",
            "Epoch 1, Batch 1651, Loss: 1.321897029876709\n",
            "Epoch 1, Batch 1661, Loss: 1.3251206874847412\n",
            "Epoch 1, Batch 1671, Loss: 1.3193482160568237\n",
            "Epoch 1, Batch 1681, Loss: 1.3166664838790894\n",
            "Epoch 1, Batch 1691, Loss: 1.3133186101913452\n",
            "Epoch 1, Batch 1701, Loss: 1.3274065256118774\n",
            "Epoch 1, Batch 1711, Loss: 1.3194468021392822\n",
            "Epoch 1, Batch 1721, Loss: 1.3143112659454346\n",
            "Epoch 1, Batch 1731, Loss: 1.3318067789077759\n",
            "Epoch 1, Batch 1741, Loss: 1.319699764251709\n",
            "Epoch 1, Batch 1751, Loss: 1.3147764205932617\n",
            "Epoch 1, Batch 1761, Loss: 1.3209410905838013\n",
            "Epoch 1, Batch 1771, Loss: 1.314854621887207\n",
            "Epoch 1, Batch 1781, Loss: 1.3161958456039429\n",
            "Epoch 1, Batch 1791, Loss: 1.340651512145996\n",
            "Epoch 1, Batch 1801, Loss: 1.3242461681365967\n",
            "Epoch 1, Batch 1811, Loss: 1.3396010398864746\n",
            "Epoch 1, Batch 1821, Loss: 1.3207752704620361\n",
            "Epoch 1, Batch 1831, Loss: 1.324050784111023\n",
            "Epoch 1, Batch 1841, Loss: 1.3276091814041138\n",
            "Epoch 1, Batch 1851, Loss: 1.3163408041000366\n",
            "Epoch 1, Batch 1861, Loss: 1.320847511291504\n",
            "Epoch 1, Batch 1871, Loss: 1.3199886083602905\n",
            "Epoch 1, Batch 1881, Loss: 1.3351467847824097\n",
            "Epoch 1, Batch 1891, Loss: 1.3150224685668945\n",
            "Epoch 1, Batch 1901, Loss: 1.349587321281433\n",
            "Epoch 1, Batch 1911, Loss: 1.314071536064148\n",
            "Epoch 1, Batch 1921, Loss: 1.3222472667694092\n",
            "Epoch 1, Batch 1931, Loss: 1.3299328088760376\n",
            "Epoch 1, Batch 1941, Loss: 1.3128669261932373\n",
            "Epoch 1, Batch 1951, Loss: 1.3225147724151611\n",
            "Epoch 1, Batch 1961, Loss: 1.331407070159912\n",
            "Epoch 1, Batch 1971, Loss: 1.3208047151565552\n",
            "Epoch 1, Batch 1981, Loss: 1.3114978075027466\n",
            "Epoch 1, Batch 1991, Loss: 1.3187286853790283\n",
            "Epoch 1, Batch 2001, Loss: 1.317183017730713\n",
            "Epoch 1, Batch 2011, Loss: 1.3206337690353394\n",
            "Epoch 1, Batch 2021, Loss: 1.3134963512420654\n",
            "Epoch 1, Batch 2031, Loss: 1.329782247543335\n",
            "Epoch 1, Batch 2041, Loss: 1.3223716020584106\n",
            "Epoch 1, Batch 2051, Loss: 1.3126575946807861\n",
            "Epoch 1, Batch 2061, Loss: 1.3402844667434692\n",
            "Epoch 1, Batch 2071, Loss: 1.3173426389694214\n",
            "Epoch 1, Batch 2081, Loss: 1.3357963562011719\n",
            "Epoch 1, Batch 2091, Loss: 1.30937922000885\n",
            "Epoch 1, Batch 2101, Loss: 1.3175002336502075\n",
            "Epoch 1, Batch 2111, Loss: 1.3190103769302368\n",
            "Epoch 1, Batch 2121, Loss: 1.3248049020767212\n",
            "Epoch 1, Batch 2131, Loss: 1.3365087509155273\n",
            "Epoch 1, Batch 2141, Loss: 1.3185136318206787\n",
            "Epoch 1, Batch 2151, Loss: 1.3155543804168701\n",
            "Epoch 1, Batch 2161, Loss: 1.318099856376648\n",
            "Epoch 1, Batch 2171, Loss: 1.3169794082641602\n",
            "Epoch 1, Batch 2181, Loss: 1.3159067630767822\n",
            "Epoch 1, Batch 2191, Loss: 1.3188790082931519\n",
            "Epoch 1, Batch 2201, Loss: 1.316362977027893\n",
            "Epoch 1, Batch 2211, Loss: 1.3142318725585938\n",
            "Epoch 1, Batch 2221, Loss: 1.3194353580474854\n",
            "Epoch 1, Batch 2231, Loss: 1.3144749402999878\n",
            "Epoch 1, Batch 2241, Loss: 1.3108863830566406\n",
            "Epoch 1, Batch 2251, Loss: 1.3163883686065674\n",
            "Epoch 1, Batch 2261, Loss: 1.3149311542510986\n",
            "Epoch 1, Batch 2271, Loss: 1.3279781341552734\n",
            "Epoch 1, Batch 2281, Loss: 1.3086364269256592\n",
            "Epoch 1, Batch 2291, Loss: 1.327306866645813\n",
            "Epoch 1, Batch 2301, Loss: 1.340131402015686\n",
            "Epoch 1, Batch 2311, Loss: 1.3137229681015015\n",
            "Epoch 1, Batch 2321, Loss: 1.3149200677871704\n",
            "Epoch 1, Batch 2331, Loss: 1.3211359977722168\n",
            "Epoch 1, Batch 2341, Loss: 1.32127046585083\n",
            "Epoch 1, Batch 2351, Loss: 1.3169677257537842\n",
            "Epoch 1, Batch 2361, Loss: 1.3146320581436157\n",
            "Epoch 1, Batch 2371, Loss: 1.3337690830230713\n",
            "Epoch 1, Batch 2381, Loss: 1.3182247877120972\n",
            "Epoch 1, Batch 2391, Loss: 1.3123414516448975\n",
            "Epoch 1, Batch 2401, Loss: 1.3162394762039185\n",
            "Epoch 1, Batch 2411, Loss: 1.3213101625442505\n",
            "Epoch 1, Batch 2421, Loss: 1.3375849723815918\n",
            "Epoch 1, Batch 2431, Loss: 1.324356198310852\n",
            "Epoch 1, Batch 2441, Loss: 1.3129535913467407\n",
            "Epoch 1, Batch 2451, Loss: 1.307728886604309\n",
            "Epoch 1, Batch 2461, Loss: 1.3188424110412598\n",
            "Epoch 1, Batch 2471, Loss: 1.3147187232971191\n",
            "Epoch 1, Batch 2481, Loss: 1.3172328472137451\n",
            "Epoch 1, Batch 2491, Loss: 1.3228274583816528\n",
            "Epoch 1, Batch 2501, Loss: 1.317945957183838\n",
            "Epoch 1, Batch 2511, Loss: 1.335085153579712\n",
            "Epoch 1, Batch 2521, Loss: 1.3363947868347168\n",
            "Epoch 1, Batch 2531, Loss: 1.3226979970932007\n",
            "Epoch 1, Batch 2541, Loss: 1.3278521299362183\n",
            "Epoch 1, Batch 2551, Loss: 1.3096330165863037\n",
            "Epoch 1, Batch 2561, Loss: 1.3147598505020142\n",
            "Epoch 1, Batch 2571, Loss: 1.309160828590393\n",
            "Epoch 1, Batch 2581, Loss: 1.3137242794036865\n",
            "Epoch 1, Batch 2591, Loss: 1.3149632215499878\n",
            "Epoch 1, Batch 2601, Loss: 1.3099321126937866\n",
            "Epoch 1, Batch 2611, Loss: 1.3195840120315552\n",
            "Epoch 1, Batch 2621, Loss: 1.336012601852417\n",
            "Epoch 1, Batch 2631, Loss: 1.3175575733184814\n",
            "Epoch 1, Batch 2641, Loss: 1.3143008947372437\n",
            "Epoch 1, Batch 2651, Loss: 1.333470106124878\n",
            "Epoch 1, Batch 2661, Loss: 1.3319824934005737\n",
            "Epoch 1, Batch 2671, Loss: 1.3195137977600098\n",
            "Epoch 1, Batch 2681, Loss: 1.3178200721740723\n",
            "Epoch 1, Batch 2691, Loss: 1.3184032440185547\n",
            "Epoch 1, Batch 2701, Loss: 1.3194113969802856\n",
            "Epoch 1, Batch 2711, Loss: 1.3087505102157593\n",
            "Epoch 1, Batch 2721, Loss: 1.3111178874969482\n",
            "Epoch 1, Batch 2731, Loss: 1.3242853879928589\n",
            "Epoch 1, Batch 2741, Loss: 1.3304201364517212\n",
            "Epoch 1, Batch 2751, Loss: 1.3114084005355835\n",
            "Epoch 1, Batch 2761, Loss: 1.3145273923873901\n",
            "Epoch 1, Batch 2771, Loss: 1.3167654275894165\n",
            "Epoch 1, Batch 2781, Loss: 1.3466026782989502\n",
            "Epoch 1, Batch 2791, Loss: 1.3070958852767944\n",
            "Epoch 1, Batch 2801, Loss: 1.3257005214691162\n",
            "Epoch 1, Batch 2811, Loss: 1.311591386795044\n",
            "Epoch 1, Batch 2821, Loss: 1.3541805744171143\n",
            "Epoch 1, Batch 2831, Loss: 1.3118036985397339\n",
            "Epoch 1, Batch 2841, Loss: 1.315619707107544\n",
            "Epoch 1, Batch 2851, Loss: 1.3271747827529907\n",
            "Epoch 1, Batch 2861, Loss: 1.3217154741287231\n",
            "Epoch 1, Batch 2871, Loss: 1.320574164390564\n",
            "Epoch 1, Batch 2881, Loss: 1.3196625709533691\n",
            "Epoch 1, Batch 2891, Loss: 1.3223786354064941\n",
            "Epoch 1, Batch 2901, Loss: 1.3476083278656006\n",
            "Epoch 1, Batch 2911, Loss: 1.3285598754882812\n",
            "Epoch 1, Batch 2921, Loss: 1.3194571733474731\n",
            "Epoch 1, Batch 2931, Loss: 1.3093899488449097\n",
            "Epoch 1, Batch 2941, Loss: 1.3319823741912842\n",
            "Epoch 1, Batch 2951, Loss: 1.3106642961502075\n",
            "Epoch 1, Batch 2961, Loss: 1.3156020641326904\n",
            "Epoch 1, Batch 2971, Loss: 1.323272466659546\n",
            "Epoch 1, Batch 2981, Loss: 1.3340495824813843\n",
            "Epoch 1, Batch 2991, Loss: 1.3386733531951904\n",
            "Epoch 1, Batch 3001, Loss: 1.3214343786239624\n",
            "Epoch 1, Batch 3011, Loss: 1.3088831901550293\n",
            "Epoch 1, Batch 3021, Loss: 1.3277500867843628\n",
            "Epoch 1, Batch 3031, Loss: 1.3168357610702515\n",
            "Epoch 1, Batch 3041, Loss: 1.3156942129135132\n",
            "Epoch 1, Batch 3051, Loss: 1.3113164901733398\n",
            "Epoch 1, Batch 3061, Loss: 1.3171833753585815\n",
            "Epoch 1, Batch 3071, Loss: 1.3166520595550537\n",
            "Epoch 1, Batch 3081, Loss: 1.3152133226394653\n",
            "Epoch 1, Batch 3091, Loss: 1.3124663829803467\n",
            "Epoch 1, Batch 3101, Loss: 1.309522271156311\n",
            "Epoch 1, Batch 3111, Loss: 1.3090165853500366\n",
            "Epoch 1, Batch 3121, Loss: 1.3352408409118652\n",
            "Epoch 1, Batch 3131, Loss: 1.323851227760315\n",
            "Epoch 1, Batch 3141, Loss: 1.3203133344650269\n",
            "Epoch 1, Batch 3151, Loss: 1.3274986743927002\n",
            "Epoch 1, Batch 3161, Loss: 1.3265494108200073\n",
            "Epoch 1, Batch 3171, Loss: 1.3195785284042358\n",
            "Epoch 1, Batch 3181, Loss: 1.3253428936004639\n",
            "Epoch 1, Batch 3191, Loss: 1.3191087245941162\n",
            "Epoch 1, Batch 3201, Loss: 1.3187662363052368\n",
            "Epoch 1, Batch 3211, Loss: 1.3132941722869873\n",
            "Epoch 1, Batch 3221, Loss: 1.3360165357589722\n",
            "Epoch 1, Batch 3231, Loss: 1.3122459650039673\n",
            "Epoch 1, Batch 3241, Loss: 1.3090052604675293\n",
            "Epoch 1, Batch 3251, Loss: 1.3284817934036255\n",
            "Epoch 1, Batch 3261, Loss: 1.3181384801864624\n",
            "Epoch 1, Batch 3271, Loss: 1.32892906665802\n",
            "Epoch 1, Batch 3281, Loss: 1.3113665580749512\n",
            "Epoch 1, Batch 3291, Loss: 1.3066301345825195\n",
            "Epoch 1, Batch 3301, Loss: 1.3147828578948975\n",
            "Epoch 1, Batch 3311, Loss: 1.3085211515426636\n",
            "Epoch 1, Batch 3321, Loss: 1.316552996635437\n",
            "Epoch 1, Batch 3331, Loss: 1.3169788122177124\n",
            "Epoch 1, Batch 3341, Loss: 1.3233762979507446\n",
            "Epoch 1, Batch 3351, Loss: 1.3248834609985352\n",
            "Epoch 1, Batch 3361, Loss: 1.3154296875\n",
            "Epoch 1, Batch 3371, Loss: 1.3149714469909668\n",
            "Epoch 1, Batch 3381, Loss: 1.3089159727096558\n",
            "Epoch 1, Batch 3391, Loss: 1.3219926357269287\n",
            "Epoch 1, Batch 3401, Loss: 1.3120002746582031\n",
            "Epoch 1, Batch 3411, Loss: 1.3157137632369995\n",
            "Epoch 1, Batch 3421, Loss: 1.321927547454834\n",
            "Epoch 1, Batch 3431, Loss: 1.31367826461792\n",
            "Epoch 1, Batch 3441, Loss: 1.3134101629257202\n",
            "Epoch 1, Batch 3451, Loss: 1.3081127405166626\n",
            "Epoch 1, Batch 3461, Loss: 1.3203448057174683\n",
            "Epoch 1, Batch 3471, Loss: 1.3080946207046509\n",
            "Epoch 1, Batch 3481, Loss: 1.3165875673294067\n",
            "Epoch 1, Batch 3491, Loss: 1.3157967329025269\n",
            "Epoch 1, Batch 3501, Loss: 1.3211944103240967\n",
            "Epoch 1, Batch 3511, Loss: 1.3075939416885376\n",
            "Epoch 1, Batch 3521, Loss: 1.3231878280639648\n",
            "Epoch 1, Batch 3531, Loss: 1.3169716596603394\n",
            "Epoch 1, Batch 3541, Loss: 1.321669340133667\n",
            "Epoch 1, Batch 3551, Loss: 1.3192322254180908\n",
            "Epoch 1, Batch 3561, Loss: 1.319322109222412\n",
            "Epoch 1, Batch 3571, Loss: 1.3074795007705688\n",
            "Epoch 1, Batch 3581, Loss: 1.3151838779449463\n",
            "Epoch 1, Batch 3591, Loss: 1.3091490268707275\n",
            "Epoch 1, Batch 3601, Loss: 1.3150372505187988\n",
            "Epoch 1, Batch 3611, Loss: 1.3182034492492676\n",
            "Epoch 1, Batch 3621, Loss: 1.3265597820281982\n",
            "Epoch 1, Batch 3631, Loss: 1.3377480506896973\n",
            "Epoch 1, Batch 3641, Loss: 1.3240529298782349\n",
            "Epoch 1, Batch 3651, Loss: 1.3110792636871338\n",
            "Epoch 1, Batch 3661, Loss: 1.314181923866272\n",
            "Epoch 1, Batch 3671, Loss: 1.3391283750534058\n",
            "Epoch 1, Batch 3681, Loss: 1.3235677480697632\n",
            "Epoch 1, Batch 3691, Loss: 1.3134346008300781\n",
            "Epoch 1, Batch 3701, Loss: 1.3288278579711914\n",
            "Epoch 1, Batch 3711, Loss: 1.3088185787200928\n",
            "Epoch 1, Batch 3721, Loss: 1.327919363975525\n",
            "Epoch 1, Batch 3731, Loss: 1.3178948163986206\n",
            "Epoch 1, Batch 3741, Loss: 1.3153400421142578\n",
            "Epoch 1, Batch 3751, Loss: 1.3212556838989258\n",
            "Epoch 1, Batch 3761, Loss: 1.3129278421401978\n",
            "Epoch 1, Batch 3771, Loss: 1.3156044483184814\n",
            "Epoch 1, Batch 3781, Loss: 1.3203377723693848\n",
            "Epoch 1, Batch 3791, Loss: 1.325581669807434\n",
            "Epoch 1, Batch 3801, Loss: 1.3278635740280151\n",
            "Epoch 1, Batch 3811, Loss: 1.314253568649292\n",
            "Epoch 1, Batch 3821, Loss: 1.312109112739563\n",
            "Epoch 1, Batch 3831, Loss: 1.313019037246704\n",
            "Epoch 1, Batch 3841, Loss: 1.325250506401062\n",
            "Epoch 1, Batch 3851, Loss: 1.3259954452514648\n",
            "Epoch 1, Batch 3861, Loss: 1.3120745420455933\n",
            "Epoch 1, Batch 3871, Loss: 1.3230456113815308\n",
            "Epoch 1, Batch 3881, Loss: 1.3147368431091309\n",
            "Epoch 1, Batch 3891, Loss: 1.3182529211044312\n",
            "Epoch 1, Batch 3901, Loss: 1.311769723892212\n",
            "Epoch 1, Batch 3911, Loss: 1.3117446899414062\n",
            "Epoch 1, Batch 3921, Loss: 1.3350119590759277\n",
            "Epoch 1, Batch 3931, Loss: 1.3105835914611816\n",
            "Epoch 1, Batch 3941, Loss: 1.3101458549499512\n",
            "Epoch 1, Batch 3951, Loss: 1.3164777755737305\n",
            "Epoch 1, Batch 3961, Loss: 1.3126758337020874\n",
            "Epoch 1, Batch 3971, Loss: 1.3103691339492798\n",
            "Epoch 1, Batch 3981, Loss: 1.3419564962387085\n",
            "Epoch 1, Batch 3991, Loss: 1.3119161128997803\n",
            "Epoch 1, Batch 4001, Loss: 1.316599726676941\n",
            "Epoch 1, Batch 4011, Loss: 1.3102593421936035\n",
            "Epoch 1, Batch 4021, Loss: 1.3427796363830566\n",
            "Epoch 1, Batch 4031, Loss: 1.318523645401001\n",
            "Epoch 1, Batch 4041, Loss: 1.3071216344833374\n",
            "Epoch 1, Batch 4051, Loss: 1.3072841167449951\n",
            "Epoch 1, Batch 4061, Loss: 1.3245810270309448\n",
            "Epoch 1, Batch 4071, Loss: 1.3181936740875244\n",
            "Epoch 1, Batch 4081, Loss: 1.3114070892333984\n",
            "Epoch 1, Batch 4091, Loss: 1.3178844451904297\n",
            "Epoch 1, Batch 4101, Loss: 1.3158270120620728\n",
            "Epoch 1, Batch 4111, Loss: 1.316803216934204\n",
            "Epoch 1, Batch 4121, Loss: 1.3163347244262695\n",
            "Epoch 1, Batch 4131, Loss: 1.3173437118530273\n",
            "Epoch 1, Batch 4141, Loss: 1.3245103359222412\n",
            "Epoch 1, Batch 4151, Loss: 1.312862515449524\n",
            "Epoch 1, Batch 4161, Loss: 1.3157484531402588\n",
            "Epoch 1, Batch 4171, Loss: 1.3174047470092773\n",
            "Epoch 1, Batch 4181, Loss: 1.3131476640701294\n",
            "Epoch 1, Batch 4191, Loss: 1.3068339824676514\n",
            "Epoch 1, Batch 4201, Loss: 1.329996943473816\n",
            "Epoch 1, Batch 4211, Loss: 1.30859375\n",
            "Epoch 1, Batch 4221, Loss: 1.307015299797058\n",
            "Epoch 1, Batch 4231, Loss: 1.3266384601593018\n",
            "Epoch 1, Batch 4241, Loss: 1.3102861642837524\n",
            "Epoch 1, Batch 4251, Loss: 1.3092831373214722\n",
            "Epoch 1, Batch 4261, Loss: 1.3165823221206665\n",
            "Epoch 1, Batch 4271, Loss: 1.3235889673233032\n",
            "Epoch 1, Batch 4281, Loss: 1.3118220567703247\n",
            "Epoch 1, Batch 4291, Loss: 1.327445387840271\n",
            "Epoch 1, Batch 4301, Loss: 1.330203890800476\n",
            "Epoch 1, Batch 4311, Loss: 1.3103916645050049\n",
            "Epoch 1, Batch 4321, Loss: 1.3267505168914795\n",
            "Epoch 1, Batch 4331, Loss: 1.306739330291748\n",
            "Epoch 1, Batch 4341, Loss: 1.3107517957687378\n",
            "Epoch 1, Batch 4351, Loss: 1.3180739879608154\n",
            "Epoch 1, Batch 4361, Loss: 1.3276671171188354\n",
            "Epoch 1, Batch 4371, Loss: 1.3127384185791016\n",
            "Epoch 1, Batch 4381, Loss: 1.3089747428894043\n",
            "Epoch 1, Batch 4391, Loss: 1.311286211013794\n",
            "Epoch 1, Batch 4401, Loss: 1.3263155221939087\n",
            "Epoch 1, Batch 4411, Loss: 1.310567855834961\n",
            "Epoch 1, Batch 4421, Loss: 1.3087600469589233\n",
            "Epoch 1, Batch 4431, Loss: 1.318359375\n",
            "Epoch 1, Batch 4441, Loss: 1.307763695716858\n",
            "Epoch 1, Batch 4451, Loss: 1.3071739673614502\n",
            "Epoch 1, Batch 4461, Loss: 1.3072751760482788\n",
            "Epoch 1, Batch 4471, Loss: 1.311200737953186\n",
            "Epoch 1, Batch 4481, Loss: 1.3210846185684204\n",
            "Epoch 1, Batch 4491, Loss: 1.3207898139953613\n",
            "Epoch 1, Batch 4501, Loss: 1.3100087642669678\n",
            "Epoch 1, Batch 4511, Loss: 1.316286325454712\n",
            "Epoch 1, Batch 4521, Loss: 1.3092988729476929\n",
            "Epoch 1, Batch 4531, Loss: 1.3278045654296875\n",
            "Epoch 1, Batch 4541, Loss: 1.3266059160232544\n",
            "Epoch 1, Batch 4551, Loss: 1.3122434616088867\n",
            "Epoch 1, Batch 4561, Loss: 1.3123042583465576\n",
            "Epoch 1, Batch 4571, Loss: 1.3450754880905151\n",
            "Epoch 1, Batch 4581, Loss: 1.3107075691223145\n",
            "Epoch 1, Batch 4591, Loss: 1.3289602994918823\n",
            "Epoch 1, Batch 4601, Loss: 1.3101372718811035\n",
            "Epoch 1, Batch 4611, Loss: 1.3271547555923462\n",
            "Epoch 1, Batch 4621, Loss: 1.3115925788879395\n",
            "Epoch 1, Batch 4631, Loss: 1.3278874158859253\n",
            "Epoch 1, Batch 4641, Loss: 1.31040620803833\n",
            "Epoch 1, Batch 4651, Loss: 1.3309308290481567\n",
            "Epoch 1, Batch 4661, Loss: 1.3089686632156372\n",
            "Epoch 1, Batch 4671, Loss: 1.3080476522445679\n",
            "Epoch 1, Batch 4681, Loss: 1.321695327758789\n",
            "Epoch 1, Batch 4691, Loss: 1.3196505308151245\n",
            "Epoch 1, Batch 4701, Loss: 1.3207824230194092\n",
            "Epoch 1, Batch 4711, Loss: 1.308149814605713\n",
            "Epoch 1, Batch 4721, Loss: 1.3201109170913696\n",
            "Epoch 1, Batch 4731, Loss: 1.3201855421066284\n",
            "Epoch 1, Batch 4741, Loss: 1.3224211931228638\n",
            "Epoch 1, Batch 4751, Loss: 1.314182996749878\n",
            "Epoch 1, Batch 4761, Loss: 1.318771243095398\n",
            "Epoch 1, Batch 4771, Loss: 1.3133153915405273\n",
            "Epoch 1, Batch 4781, Loss: 1.323238730430603\n",
            "Epoch 1, Batch 4791, Loss: 1.312564730644226\n",
            "Epoch 1, Batch 4801, Loss: 1.3317521810531616\n",
            "Epoch 1, Batch 4811, Loss: 1.3197256326675415\n",
            "Epoch 1, Batch 4821, Loss: 1.3050023317337036\n",
            "Epoch 1, Batch 4831, Loss: 1.3287246227264404\n",
            "Epoch 1, Batch 4841, Loss: 1.3198580741882324\n",
            "Epoch 1, Batch 4851, Loss: 1.307899832725525\n",
            "Epoch 1, Batch 4861, Loss: 1.3082016706466675\n",
            "Epoch 1, Batch 4871, Loss: 1.3182612657546997\n",
            "Epoch 1, Batch 4881, Loss: 1.3084214925765991\n",
            "Epoch 1, Batch 4891, Loss: 1.3100571632385254\n",
            "Epoch 1, Batch 4901, Loss: 1.3197574615478516\n",
            "Epoch 1, Batch 4911, Loss: 1.3112760782241821\n",
            "Epoch 1, Batch 4921, Loss: 1.3264257907867432\n",
            "Epoch 1, Batch 4931, Loss: 1.308156132698059\n",
            "Epoch 1, Batch 4941, Loss: 1.318964958190918\n",
            "Epoch 1, Batch 4951, Loss: 1.3163083791732788\n",
            "Epoch 1, Batch 4961, Loss: 1.3135712146759033\n",
            "Epoch 1, Batch 4971, Loss: 1.3127552270889282\n",
            "Epoch 1, Batch 4981, Loss: 1.3246712684631348\n",
            "Epoch 1, Batch 4991, Loss: 1.3216086626052856\n",
            "Epoch 1, Batch 5001, Loss: 1.3075448274612427\n",
            "Epoch 1, Batch 5011, Loss: 1.3172497749328613\n",
            "Epoch 1, Batch 5021, Loss: 1.311195731163025\n",
            "Epoch 1, Batch 5031, Loss: 1.3143872022628784\n",
            "Epoch 1, Batch 5041, Loss: 1.3163254261016846\n",
            "Epoch 1, Batch 5051, Loss: 1.3235752582550049\n",
            "Epoch 1, Batch 5061, Loss: 1.3093185424804688\n",
            "Epoch 1, Batch 5071, Loss: 1.3150978088378906\n",
            "Epoch 1, Batch 5081, Loss: 1.3174177408218384\n",
            "Epoch 1, Batch 5091, Loss: 1.3187099695205688\n",
            "Epoch 1, Batch 5101, Loss: 1.3314824104309082\n",
            "Epoch 1, Batch 5111, Loss: 1.3161635398864746\n",
            "Epoch 1, Batch 5121, Loss: 1.3194857835769653\n",
            "Epoch 1, Batch 5131, Loss: 1.3156654834747314\n",
            "Epoch 1, Batch 5141, Loss: 1.3130913972854614\n",
            "Epoch 1, Batch 5151, Loss: 1.3319779634475708\n",
            "Epoch 1, Batch 5161, Loss: 1.3401669263839722\n",
            "Epoch 1, Batch 5171, Loss: 1.337342381477356\n",
            "Epoch 1, Batch 5181, Loss: 1.3295892477035522\n",
            "Epoch 1, Batch 5191, Loss: 1.3181535005569458\n",
            "Epoch 1, Batch 5201, Loss: 1.335818886756897\n",
            "Epoch 1, Batch 5211, Loss: 1.3384528160095215\n",
            "Epoch 1, Batch 5221, Loss: 1.3231033086776733\n",
            "Epoch 1, Batch 5231, Loss: 1.3211591243743896\n",
            "Epoch 1, Batch 5241, Loss: 1.3139846324920654\n",
            "Epoch 1, Batch 5251, Loss: 1.3173948526382446\n",
            "Epoch 1, Batch 5261, Loss: 1.311644196510315\n",
            "Epoch 1, Batch 5271, Loss: 1.3288285732269287\n",
            "Epoch 1, Batch 5281, Loss: 1.309429407119751\n",
            "Epoch 1, Batch 5291, Loss: 1.316515564918518\n",
            "Epoch 1, Batch 5301, Loss: 1.311263084411621\n",
            "Epoch 1, Batch 5311, Loss: 1.316409707069397\n",
            "Epoch 1, Batch 5321, Loss: 1.3144389390945435\n",
            "Epoch 1, Batch 5331, Loss: 1.3150321245193481\n",
            "Epoch 1, Batch 5341, Loss: 1.3197808265686035\n",
            "Epoch 1, Batch 5351, Loss: 1.3092567920684814\n",
            "Epoch 1, Batch 5361, Loss: 1.3130789995193481\n",
            "Epoch 1, Batch 5371, Loss: 1.3145689964294434\n",
            "Epoch 1, Batch 5381, Loss: 1.329805850982666\n",
            "Epoch 1, Batch 5391, Loss: 1.3209909200668335\n",
            "Epoch 1, Batch 5401, Loss: 1.3040624856948853\n",
            "Epoch 1, Batch 5411, Loss: 1.3131357431411743\n",
            "Epoch 1, Batch 5421, Loss: 1.3431559801101685\n",
            "Epoch 1, Batch 5431, Loss: 1.3200509548187256\n",
            "Epoch 1, Batch 5441, Loss: 1.3061238527297974\n",
            "Epoch 1, Batch 5451, Loss: 1.3261460065841675\n",
            "Epoch 1, Batch 5461, Loss: 1.3146597146987915\n",
            "Epoch 1, Batch 5471, Loss: 1.3336678743362427\n",
            "Epoch 1, Batch 5481, Loss: 1.3166428804397583\n",
            "Epoch 1, Batch 5491, Loss: 1.3071484565734863\n",
            "Epoch 1, Batch 5501, Loss: 1.3072292804718018\n",
            "Epoch 1, Batch 5511, Loss: 1.3155323266983032\n",
            "Epoch 1, Batch 5521, Loss: 1.3255717754364014\n",
            "Epoch 1, Batch 5531, Loss: 1.3156898021697998\n",
            "Epoch 1, Batch 5541, Loss: 1.3057100772857666\n",
            "Epoch 1, Batch 5551, Loss: 1.3136231899261475\n",
            "Epoch 1, Batch 5561, Loss: 1.3125158548355103\n",
            "Epoch 1, Batch 5571, Loss: 1.315892219543457\n",
            "Epoch 1, Batch 5581, Loss: 1.3091964721679688\n",
            "Epoch 1, Batch 5591, Loss: 1.321033000946045\n",
            "Epoch 1, Batch 5601, Loss: 1.3088937997817993\n",
            "Epoch 1, Batch 5611, Loss: 1.3122209310531616\n",
            "Epoch 1, Batch 5621, Loss: 1.3391203880310059\n",
            "Epoch 1, Batch 5631, Loss: 1.3250608444213867\n",
            "Epoch 1, Batch 5641, Loss: 1.3440190553665161\n",
            "Epoch 1, Batch 5651, Loss: 1.3414130210876465\n",
            "Epoch 1, Batch 5661, Loss: 1.3057904243469238\n",
            "Epoch 1, Batch 5671, Loss: 1.3144317865371704\n",
            "Epoch 1, Batch 5681, Loss: 1.3226420879364014\n",
            "Epoch 1, Batch 5691, Loss: 1.324938416481018\n",
            "Epoch 1, Batch 5701, Loss: 1.3071324825286865\n",
            "Epoch 1, Batch 5711, Loss: 1.3228461742401123\n",
            "Epoch 1, Batch 5721, Loss: 1.3141621351242065\n",
            "Epoch 1, Batch 5731, Loss: 1.3118958473205566\n",
            "Epoch 1, Batch 5741, Loss: 1.30912446975708\n",
            "Epoch 1, Batch 5751, Loss: 1.3290704488754272\n",
            "Epoch 1, Batch 5761, Loss: 1.3213788270950317\n",
            "Epoch 1, Batch 5771, Loss: 1.3190048933029175\n",
            "Epoch 1, Batch 5781, Loss: 1.320082664489746\n",
            "Epoch 1, Batch 5791, Loss: 1.3286038637161255\n",
            "Epoch 1, Batch 5801, Loss: 1.3189903497695923\n",
            "Epoch 1, Batch 5811, Loss: 1.3109538555145264\n",
            "Epoch 1, Batch 5821, Loss: 1.311293363571167\n",
            "Epoch 1, Batch 5831, Loss: 1.313124418258667\n",
            "Epoch 1, Batch 5841, Loss: 1.311426043510437\n",
            "Epoch 1, Batch 5851, Loss: 1.3219319581985474\n",
            "Epoch 1, Batch 5861, Loss: 1.3478232622146606\n",
            "Epoch 1, Batch 5871, Loss: 1.4393970966339111\n",
            "Epoch 1, Batch 5881, Loss: 1.403619408607483\n",
            "Epoch 1, Batch 5891, Loss: 1.3722813129425049\n",
            "Epoch 1, Batch 5901, Loss: 1.3548429012298584\n",
            "Epoch 1, Batch 5911, Loss: 1.3386085033416748\n",
            "Epoch 1, Batch 5921, Loss: 1.361778736114502\n",
            "Epoch 1, Batch 5931, Loss: 1.515741229057312\n",
            "Epoch 1, Batch 5941, Loss: 1.5021804571151733\n",
            "Epoch 1, Batch 5951, Loss: 1.455541968345642\n",
            "Epoch 1, Batch 5961, Loss: 1.4245154857635498\n",
            "Epoch 1, Batch 5971, Loss: 1.4056422710418701\n",
            "Epoch 1, Batch 5981, Loss: 1.392814040184021\n",
            "Epoch 1, Batch 5991, Loss: 1.3928624391555786\n",
            "Epoch 1, Batch 6001, Loss: 1.3968045711517334\n",
            "Epoch 1, Batch 6011, Loss: 1.3849855661392212\n",
            "Epoch 1, Batch 6021, Loss: 1.3632562160491943\n",
            "Epoch 1, Batch 6031, Loss: 1.3767883777618408\n",
            "Epoch 1, Batch 6041, Loss: 1.3590983152389526\n",
            "Epoch 1, Batch 6051, Loss: 1.3864425420761108\n",
            "Epoch 1, Batch 6061, Loss: 1.3646414279937744\n",
            "Epoch 1, Batch 6071, Loss: 1.3556623458862305\n",
            "Epoch 1, Batch 6081, Loss: 1.3485063314437866\n",
            "Epoch 1, Batch 6091, Loss: 1.356536626815796\n",
            "Epoch 1, Batch 6101, Loss: 1.36259925365448\n",
            "Epoch 1, Batch 6111, Loss: 1.3386250734329224\n",
            "Epoch 1, Batch 6121, Loss: 1.3401142358779907\n",
            "Epoch 1, Batch 6131, Loss: 1.3341854810714722\n",
            "Epoch 1, Batch 6141, Loss: 1.3395617008209229\n",
            "Epoch 1, Batch 6151, Loss: 1.3472399711608887\n",
            "Epoch 1, Batch 6161, Loss: 1.341190218925476\n",
            "Epoch 1, Batch 6171, Loss: 1.3640294075012207\n",
            "Epoch 1, Batch 6181, Loss: 1.3439637422561646\n",
            "Epoch 1, Batch 6191, Loss: 1.358317494392395\n",
            "Epoch 1, Batch 6201, Loss: 1.340449571609497\n",
            "Epoch 1, Batch 6211, Loss: 1.3467016220092773\n",
            "Epoch 1, Batch 6221, Loss: 1.3513063192367554\n",
            "Epoch 1, Batch 6231, Loss: 1.3534365892410278\n",
            "Epoch 1, Batch 6241, Loss: 1.339760184288025\n",
            "Epoch 1, Batch 6251, Loss: 1.3340259790420532\n",
            "Epoch 1, Batch 6261, Loss: 1.3309870958328247\n",
            "Epoch 1, Batch 6271, Loss: 1.336568832397461\n",
            "Epoch 1, Batch 6281, Loss: 1.3462693691253662\n",
            "Epoch 1, Batch 6291, Loss: 1.3457916975021362\n",
            "Epoch 1, Batch 6301, Loss: 1.3489139080047607\n",
            "Epoch 1, Batch 6311, Loss: 1.3419362306594849\n",
            "Epoch 1, Batch 6321, Loss: 1.334869623184204\n",
            "Epoch 1, Batch 6331, Loss: 1.336672306060791\n",
            "Epoch 1, Batch 6341, Loss: 1.3430461883544922\n",
            "Epoch 1, Batch 6351, Loss: 1.3252394199371338\n",
            "Epoch 1, Batch 6361, Loss: 1.325242042541504\n",
            "Epoch 1, Batch 6371, Loss: 1.3181750774383545\n",
            "Epoch 1, Batch 6381, Loss: 1.3278886079788208\n",
            "Epoch 1, Batch 6391, Loss: 1.3264491558074951\n",
            "Epoch 1, Batch 6401, Loss: 1.3280127048492432\n",
            "Epoch 1, Batch 6411, Loss: 1.3328384160995483\n",
            "Epoch 1, Batch 6421, Loss: 1.3243916034698486\n",
            "Epoch 1, Batch 6431, Loss: 1.3278628587722778\n",
            "Epoch 1, Batch 6441, Loss: 1.324315071105957\n",
            "Epoch 1, Batch 6451, Loss: 1.3324260711669922\n",
            "Epoch 1, Batch 6461, Loss: 1.3270245790481567\n",
            "Epoch 1, Batch 6471, Loss: 1.3431711196899414\n",
            "Epoch 1, Batch 6481, Loss: 1.3312572240829468\n",
            "Epoch 1, Batch 6491, Loss: 1.3340322971343994\n",
            "Epoch 1, Batch 6501, Loss: 1.3353824615478516\n",
            "Epoch 1, Batch 6511, Loss: 1.3344755172729492\n",
            "Epoch 1, Batch 6521, Loss: 1.325325608253479\n",
            "Epoch 1, Batch 6531, Loss: 1.3179430961608887\n",
            "Epoch 1, Batch 6541, Loss: 1.3142694234848022\n",
            "Epoch 1, Batch 6551, Loss: 1.3543728590011597\n",
            "Epoch 1, Batch 6561, Loss: 1.326529622077942\n",
            "Epoch 1, Batch 6571, Loss: 1.3259786367416382\n",
            "Epoch 1, Batch 6581, Loss: 1.330697774887085\n",
            "Epoch 1, Batch 6591, Loss: 1.3191204071044922\n",
            "Epoch 1, Batch 6601, Loss: 1.3300901651382446\n",
            "Epoch 1, Batch 6611, Loss: 1.324512243270874\n",
            "Epoch 1, Batch 6621, Loss: 1.3318078517913818\n",
            "Epoch 1, Batch 6631, Loss: 1.3287478685379028\n",
            "Epoch 1, Batch 6641, Loss: 1.3278263807296753\n",
            "Epoch 1, Batch 6651, Loss: 1.3237907886505127\n",
            "Epoch 1, Batch 6661, Loss: 1.3477038145065308\n",
            "Epoch 1, Batch 6671, Loss: 1.3307338953018188\n",
            "Epoch 1, Batch 6681, Loss: 1.3294638395309448\n",
            "Epoch 1, Batch 6691, Loss: 1.3360497951507568\n",
            "Epoch 1, Batch 6701, Loss: 1.3307533264160156\n",
            "Epoch 1, Batch 6711, Loss: 1.3211880922317505\n",
            "Epoch 1, Batch 6721, Loss: 1.3253657817840576\n",
            "Epoch 1, Batch 6731, Loss: 1.340607762336731\n",
            "Epoch 1, Batch 6741, Loss: 1.320636510848999\n",
            "Epoch 1, Batch 6751, Loss: 1.3176623582839966\n",
            "Epoch 1, Batch 6761, Loss: 1.3313567638397217\n",
            "Epoch 1, Batch 6771, Loss: 1.3404285907745361\n",
            "Epoch 1, Batch 6781, Loss: 1.322214961051941\n",
            "Epoch 1, Batch 6791, Loss: 1.3387598991394043\n",
            "Epoch 1, Batch 6801, Loss: 1.3238650560379028\n",
            "Epoch 1, Batch 6811, Loss: 1.3147484064102173\n",
            "Epoch 1, Batch 6821, Loss: 1.3194111585617065\n",
            "Epoch 1, Batch 6831, Loss: 1.327056884765625\n",
            "Epoch 1, Batch 6841, Loss: 1.3258341550827026\n",
            "Epoch 1, Batch 6851, Loss: 1.3235315084457397\n",
            "Epoch 1, Batch 6861, Loss: 1.3172972202301025\n",
            "Epoch 1, Batch 6871, Loss: 1.3139021396636963\n",
            "Epoch 1, Batch 6881, Loss: 1.324491262435913\n",
            "Epoch 1, Batch 6891, Loss: 1.3311783075332642\n",
            "Epoch 1, Batch 6901, Loss: 1.3162281513214111\n",
            "Epoch 1, Batch 6911, Loss: 1.3136868476867676\n",
            "Epoch 1, Batch 6921, Loss: 1.315606951713562\n",
            "Epoch 1, Batch 6931, Loss: 1.3239681720733643\n",
            "Epoch 1, Batch 6941, Loss: 1.319166660308838\n",
            "Epoch 1, Batch 6951, Loss: 1.3151553869247437\n",
            "Epoch 1, Batch 6961, Loss: 1.326919674873352\n",
            "Epoch 1, Batch 6971, Loss: 1.3132935762405396\n",
            "Epoch 1, Batch 6981, Loss: 1.332837700843811\n",
            "Epoch 1, Batch 6991, Loss: 1.3333382606506348\n",
            "Epoch 1, Batch 7001, Loss: 1.3189021348953247\n",
            "Epoch 1, Batch 7011, Loss: 1.3303676843643188\n",
            "Epoch 1, Batch 7021, Loss: 1.3298834562301636\n",
            "Epoch 1, Batch 7031, Loss: 1.3370782136917114\n",
            "Epoch 1, Batch 7041, Loss: 1.3104270696640015\n",
            "Epoch 1, Batch 7051, Loss: 1.3138750791549683\n",
            "Epoch 1, Batch 7061, Loss: 1.3124738931655884\n",
            "Epoch 1, Batch 7071, Loss: 1.3182814121246338\n",
            "Epoch 1, Batch 7081, Loss: 1.3337677717208862\n",
            "Epoch 1, Batch 7091, Loss: 1.3202898502349854\n",
            "Epoch 1, Batch 7101, Loss: 1.3255194425582886\n",
            "Epoch 1, Batch 7111, Loss: 1.3411906957626343\n",
            "Epoch 1, Batch 7121, Loss: 1.317740797996521\n",
            "Epoch 1, Batch 7131, Loss: 1.3189550638198853\n",
            "Epoch 1, Batch 7141, Loss: 1.314982533454895\n",
            "Epoch 1, Batch 7151, Loss: 1.3246835470199585\n",
            "Epoch 1, Batch 7161, Loss: 1.330300211906433\n",
            "Epoch 1, Batch 7171, Loss: 1.3207590579986572\n",
            "Epoch 1, Batch 7181, Loss: 1.313034176826477\n",
            "Epoch 1, Batch 7191, Loss: 1.3113776445388794\n",
            "Epoch 1, Batch 7201, Loss: 1.3370907306671143\n",
            "Epoch 1, Batch 7211, Loss: 1.3092453479766846\n",
            "Epoch 1, Batch 7221, Loss: 1.3176935911178589\n",
            "Epoch 1, Batch 7231, Loss: 1.3294039964675903\n",
            "Epoch 1, Batch 7241, Loss: 1.3213191032409668\n",
            "Epoch 1, Batch 7251, Loss: 1.3113794326782227\n",
            "Epoch 1, Batch 7261, Loss: 1.3124364614486694\n",
            "Epoch 1, Batch 7271, Loss: 1.325792670249939\n",
            "Epoch 1, Batch 7281, Loss: 1.3288543224334717\n",
            "Epoch 1, Batch 7291, Loss: 1.3278541564941406\n",
            "Epoch 1, Batch 7301, Loss: 1.3086025714874268\n",
            "Epoch 1, Batch 7311, Loss: 1.3120266199111938\n",
            "Epoch 1, Batch 7321, Loss: 1.32498037815094\n",
            "Epoch 1, Batch 7331, Loss: 1.320103406906128\n",
            "Epoch 1, Batch 7341, Loss: 1.3258333206176758\n",
            "Epoch 1, Batch 7351, Loss: 1.3160046339035034\n",
            "Epoch 1, Batch 7361, Loss: 1.3111058473587036\n",
            "Epoch 1, Batch 7371, Loss: 1.3238334655761719\n",
            "Epoch 1, Batch 7381, Loss: 1.3337494134902954\n",
            "Epoch 1, Batch 7391, Loss: 1.3396406173706055\n",
            "Epoch 1, Batch 7401, Loss: 1.3128446340560913\n",
            "Epoch 1, Batch 7411, Loss: 1.3178057670593262\n",
            "Epoch 1, Batch 7421, Loss: 1.3267526626586914\n",
            "Epoch 1, Batch 7431, Loss: 1.3207348585128784\n",
            "Epoch 1, Batch 7441, Loss: 1.3703950643539429\n",
            "Epoch 1, Batch 7451, Loss: 1.3315415382385254\n",
            "Epoch 1, Batch 7461, Loss: 1.3115715980529785\n",
            "Epoch 1, Batch 7471, Loss: 1.3221279382705688\n",
            "Epoch 1, Batch 7481, Loss: 1.3193224668502808\n",
            "Epoch 1, Batch 7491, Loss: 1.3459802865982056\n",
            "Epoch 1, Batch 7501, Loss: 1.3278679847717285\n",
            "Epoch 1, Batch 7511, Loss: 1.32630455493927\n",
            "Epoch 1, Batch 7521, Loss: 1.3247568607330322\n",
            "Epoch 1, Batch 7531, Loss: 1.3189698457717896\n",
            "Epoch 1, Batch 7541, Loss: 1.3131685256958008\n",
            "Epoch 1, Batch 7551, Loss: 1.3078858852386475\n",
            "Epoch 1, Batch 7561, Loss: 1.316850185394287\n",
            "Epoch 1, Batch 7571, Loss: 1.3150924444198608\n",
            "Epoch 1, Batch 7581, Loss: 1.3154253959655762\n",
            "Epoch 1, Batch 7591, Loss: 1.3261116743087769\n",
            "Epoch 1, Batch 7601, Loss: 1.3149956464767456\n",
            "Epoch 1, Batch 7611, Loss: 1.3102810382843018\n",
            "Epoch 1, Batch 7621, Loss: 1.306228518486023\n",
            "Epoch 1, Batch 7631, Loss: 1.3279283046722412\n",
            "Epoch 1, Batch 7641, Loss: 1.3264034986495972\n",
            "Epoch 1, Batch 7651, Loss: 1.3124151229858398\n",
            "Epoch 1, Batch 7661, Loss: 1.3313605785369873\n",
            "Epoch 1, Batch 7671, Loss: 1.3093470335006714\n",
            "Epoch 1, Batch 7681, Loss: 1.3089394569396973\n",
            "Epoch 1, Batch 7691, Loss: 1.319467306137085\n",
            "Epoch 1, Batch 7701, Loss: 1.3204925060272217\n",
            "Epoch 1, Batch 7711, Loss: 1.327168583869934\n",
            "Epoch 1, Batch 7721, Loss: 1.3068915605545044\n",
            "Epoch 1, Batch 7731, Loss: 1.3254575729370117\n",
            "Epoch 1, Batch 7741, Loss: 1.3171025514602661\n",
            "Epoch 1, Batch 7751, Loss: 1.3232619762420654\n",
            "Epoch 1, Batch 7761, Loss: 1.317480206489563\n",
            "Epoch 1, Batch 7771, Loss: 1.3148094415664673\n",
            "Epoch 1, Batch 7781, Loss: 1.3170182704925537\n",
            "Epoch 1, Batch 7791, Loss: 1.3521357774734497\n",
            "Epoch 1, Batch 7801, Loss: 1.3077951669692993\n",
            "Epoch 1, Batch 7811, Loss: 1.3117318153381348\n",
            "Epoch 1, Batch 7821, Loss: 1.3226910829544067\n",
            "Epoch 1, Batch 7831, Loss: 1.3319549560546875\n",
            "Epoch 1, Batch 7841, Loss: 1.3060578107833862\n",
            "Epoch 1, Batch 7851, Loss: 1.308007836341858\n",
            "Epoch 1, Batch 7861, Loss: 1.3157556056976318\n",
            "Epoch 1, Batch 7871, Loss: 1.3309366703033447\n",
            "Epoch 1, Batch 7881, Loss: 1.3085124492645264\n",
            "Epoch 1, Batch 7891, Loss: 1.3131448030471802\n",
            "Epoch 1, Batch 7901, Loss: 1.3317939043045044\n",
            "Epoch 1, Batch 7911, Loss: 1.3199167251586914\n",
            "Epoch 1, Batch 7921, Loss: 1.3381223678588867\n",
            "Epoch 1, Batch 7931, Loss: 1.3224490880966187\n",
            "Epoch 1, Batch 7941, Loss: 1.3308902978897095\n",
            "Epoch 1, Batch 7951, Loss: 1.3118884563446045\n",
            "Epoch 1, Batch 7961, Loss: 1.3173686265945435\n",
            "Epoch 1, Batch 7971, Loss: 1.3455759286880493\n",
            "Epoch 1, Batch 7981, Loss: 1.317484974861145\n",
            "Epoch 1, Batch 7991, Loss: 1.3096643686294556\n",
            "Epoch 1, Batch 8001, Loss: 1.3346318006515503\n",
            "Epoch 1, Batch 8011, Loss: 1.312644124031067\n",
            "Epoch 1, Batch 8021, Loss: 1.3166003227233887\n",
            "Epoch 1, Batch 8031, Loss: 1.3184796571731567\n",
            "Epoch 1, Batch 8041, Loss: 1.3113489151000977\n",
            "Epoch 1, Batch 8051, Loss: 1.312751293182373\n",
            "Epoch 1, Batch 8061, Loss: 1.3152037858963013\n",
            "Epoch 1, Batch 8071, Loss: 1.3164104223251343\n",
            "Epoch 1, Batch 8081, Loss: 1.317888855934143\n",
            "Epoch 1, Batch 8091, Loss: 1.3106136322021484\n",
            "Epoch 1, Batch 8101, Loss: 1.3185638189315796\n",
            "Epoch 1, Batch 8111, Loss: 1.3092349767684937\n",
            "Epoch 1, Batch 8121, Loss: 1.3044458627700806\n",
            "Epoch 1, Batch 8131, Loss: 1.3353948593139648\n",
            "Epoch 1, Batch 8141, Loss: 1.3120003938674927\n",
            "Epoch 1, Batch 8151, Loss: 1.3138997554779053\n",
            "Epoch 1, Batch 8161, Loss: 1.3178600072860718\n",
            "Epoch 1, Batch 8171, Loss: 1.3274784088134766\n",
            "Epoch 1, Batch 8181, Loss: 1.311444640159607\n",
            "Epoch 1, Batch 8191, Loss: 1.3053820133209229\n",
            "Epoch 1, Batch 8201, Loss: 1.310846209526062\n",
            "Epoch 1, Batch 8211, Loss: 1.3060119152069092\n",
            "Epoch 1, Batch 8221, Loss: 1.3080717325210571\n",
            "Epoch 1, Batch 8231, Loss: 1.3168326616287231\n",
            "Epoch 1, Batch 8241, Loss: 1.320061206817627\n",
            "Epoch 1, Batch 8251, Loss: 1.312016248703003\n",
            "Epoch 1, Batch 8261, Loss: 1.328819751739502\n",
            "Epoch 1, Batch 8271, Loss: 1.308681607246399\n",
            "Epoch 1, Batch 8281, Loss: 1.3095279932022095\n",
            "Epoch 1, Batch 8291, Loss: 1.3205653429031372\n",
            "Epoch 1, Batch 8301, Loss: 1.3154010772705078\n",
            "Epoch 1, Batch 8311, Loss: 1.3161075115203857\n",
            "Epoch 1, Batch 8321, Loss: 1.3148764371871948\n",
            "Epoch 1, Batch 8331, Loss: 1.3165212869644165\n",
            "Epoch 1, Batch 8341, Loss: 1.3082950115203857\n",
            "Epoch 1, Batch 8351, Loss: 1.3270225524902344\n",
            "Epoch 1, Batch 8361, Loss: 1.3107599020004272\n",
            "Epoch 1, Batch 8371, Loss: 1.3355971574783325\n",
            "Epoch 1, Batch 8381, Loss: 1.3505487442016602\n",
            "Epoch 1, Batch 8391, Loss: 1.3097177743911743\n",
            "Epoch 1, Batch 8401, Loss: 1.3358328342437744\n",
            "Epoch 1, Batch 8411, Loss: 1.3143370151519775\n",
            "Epoch 1, Batch 8421, Loss: 1.3161207437515259\n",
            "Epoch 1, Batch 8431, Loss: 1.3263522386550903\n",
            "Epoch 1, Batch 8441, Loss: 1.3050823211669922\n",
            "Epoch 1, Batch 8451, Loss: 1.3197507858276367\n",
            "Epoch 1, Batch 8461, Loss: 1.3174662590026855\n",
            "Epoch 1, Batch 8471, Loss: 1.3394914865493774\n",
            "Epoch 1, Batch 8481, Loss: 1.3342056274414062\n",
            "Epoch 1, Batch 8491, Loss: 1.3209227323532104\n",
            "Epoch 1, Batch 8501, Loss: 1.3310619592666626\n",
            "Epoch 1, Batch 8511, Loss: 1.3091481924057007\n",
            "Epoch 1, Batch 8521, Loss: 1.3054463863372803\n",
            "Epoch 1, Batch 8531, Loss: 1.3103275299072266\n",
            "Epoch 1, Batch 8541, Loss: 1.3151030540466309\n",
            "Epoch 1, Batch 8551, Loss: 1.3116810321807861\n",
            "Epoch 1, Batch 8561, Loss: 1.3298996686935425\n",
            "Epoch 1, Batch 8571, Loss: 1.3187979459762573\n",
            "Epoch 1, Batch 8581, Loss: 1.3167470693588257\n",
            "Epoch 1, Batch 8591, Loss: 1.3122265338897705\n",
            "Epoch 1, Batch 8601, Loss: 1.3231775760650635\n",
            "Epoch 1, Batch 8611, Loss: 1.317025065422058\n",
            "Epoch 1, Batch 8621, Loss: 1.321088194847107\n",
            "Epoch 1, Batch 8631, Loss: 1.3270902633666992\n",
            "Epoch 1, Batch 8641, Loss: 1.3309226036071777\n",
            "Epoch 1, Batch 8651, Loss: 1.3051984310150146\n",
            "Epoch 1, Batch 8661, Loss: 1.327418327331543\n",
            "Epoch 1, Batch 8671, Loss: 1.3069294691085815\n",
            "Epoch 1, Batch 8681, Loss: 1.3135255575180054\n",
            "Epoch 1, Batch 8691, Loss: 1.3057587146759033\n",
            "Epoch 1, Batch 8701, Loss: 1.3220840692520142\n",
            "Epoch 1, Batch 8711, Loss: 1.3157658576965332\n",
            "Epoch 1, Batch 8721, Loss: 1.3093101978302002\n",
            "Epoch 1, Batch 8731, Loss: 1.3039871454238892\n",
            "Epoch 1, Batch 8741, Loss: 1.3054877519607544\n",
            "Epoch 1, Batch 8751, Loss: 1.320485234260559\n",
            "Epoch 1, Batch 8761, Loss: 1.3168095350265503\n",
            "Epoch 1, Batch 8771, Loss: 1.3097018003463745\n",
            "Epoch 1, Batch 8781, Loss: 1.3144762516021729\n",
            "Epoch 1, Batch 8791, Loss: 1.317011833190918\n",
            "Epoch 1, Batch 8801, Loss: 1.3125866651535034\n",
            "Epoch 1, Batch 8811, Loss: 1.3097279071807861\n",
            "Epoch 1, Batch 8821, Loss: 1.3121861219406128\n",
            "Epoch 1, Batch 8831, Loss: 1.3238033056259155\n",
            "Epoch 1, Batch 8841, Loss: 1.3400276899337769\n",
            "Epoch 1, Batch 8851, Loss: 1.3198914527893066\n",
            "Epoch 1, Batch 8861, Loss: 1.312174916267395\n",
            "Epoch 1, Batch 8871, Loss: 1.3119075298309326\n",
            "Epoch 1, Batch 8881, Loss: 1.3104312419891357\n",
            "Epoch 1, Batch 8891, Loss: 1.322828769683838\n",
            "Epoch 1, Batch 8901, Loss: 1.3111000061035156\n",
            "Epoch 1, Batch 8911, Loss: 1.3220345973968506\n",
            "Epoch 1, Batch 8921, Loss: 1.3056086301803589\n",
            "Epoch 1, Batch 8931, Loss: 1.3139064311981201\n",
            "Epoch 1, Batch 8941, Loss: 1.3300532102584839\n",
            "Epoch 1, Batch 8951, Loss: 1.3104573488235474\n",
            "Epoch 1, Batch 8961, Loss: 1.3100084066390991\n",
            "Epoch 1, Batch 8971, Loss: 1.3151278495788574\n",
            "Epoch 1, Batch 8981, Loss: 1.3070718050003052\n",
            "Epoch 1, Batch 8991, Loss: 1.3096078634262085\n",
            "Epoch 1, Batch 9001, Loss: 1.3294544219970703\n",
            "Epoch 1, Batch 9011, Loss: 1.3135044574737549\n",
            "Epoch 1, Batch 9021, Loss: 1.318274736404419\n",
            "Epoch 1, Batch 9031, Loss: 1.3083933591842651\n",
            "Epoch 1, Batch 9041, Loss: 1.3067903518676758\n",
            "Epoch 1, Batch 9051, Loss: 1.313478708267212\n",
            "Epoch 1, Batch 9061, Loss: 1.3131721019744873\n",
            "Epoch 1, Batch 9071, Loss: 1.3093783855438232\n",
            "Epoch 1, Batch 9081, Loss: 1.3081945180892944\n",
            "Epoch 1, Batch 9091, Loss: 1.3093831539154053\n",
            "Epoch 1, Batch 9101, Loss: 1.332017183303833\n",
            "Epoch 1, Batch 9111, Loss: 1.3036577701568604\n",
            "Epoch 1, Batch 9121, Loss: 1.3098992109298706\n",
            "Epoch 1, Batch 9131, Loss: 1.3091351985931396\n",
            "Epoch 1, Batch 9141, Loss: 1.3176921606063843\n",
            "Epoch 1, Batch 9151, Loss: 1.315596103668213\n",
            "Epoch 1, Batch 9161, Loss: 1.3190343379974365\n",
            "Epoch 1, Batch 9171, Loss: 1.3216146230697632\n",
            "Epoch 1, Batch 9181, Loss: 1.3113211393356323\n",
            "Epoch 1, Batch 9191, Loss: 1.3282197713851929\n",
            "Epoch 1, Batch 9201, Loss: 1.3049724102020264\n",
            "Epoch 1, Batch 9211, Loss: 1.3224931955337524\n",
            "Epoch 1, Batch 9221, Loss: 1.3111964464187622\n",
            "Epoch 1, Batch 9231, Loss: 1.30552077293396\n",
            "Epoch 1, Batch 9241, Loss: 1.319732427597046\n",
            "Epoch 1, Batch 9251, Loss: 1.3077759742736816\n",
            "Epoch 1, Batch 9261, Loss: 1.3081731796264648\n",
            "Epoch 1, Batch 9271, Loss: 1.3175431489944458\n",
            "Epoch 1, Batch 9281, Loss: 1.314756155014038\n",
            "Epoch 1, Batch 9291, Loss: 1.3205740451812744\n",
            "Epoch 1, Batch 9301, Loss: 1.315006971359253\n",
            "Epoch 1, Batch 9311, Loss: 1.310592532157898\n",
            "Epoch 1, Batch 9321, Loss: 1.3137151002883911\n",
            "Epoch 1, Batch 9331, Loss: 1.3082797527313232\n",
            "Epoch 1, Batch 9341, Loss: 1.3234082460403442\n",
            "Epoch 1, Batch 9351, Loss: 1.303787350654602\n",
            "Epoch 1, Batch 9361, Loss: 1.3185920715332031\n",
            "Epoch 1, Batch 9371, Loss: 1.3291442394256592\n",
            "Epoch 1, Batch 9381, Loss: 1.3170857429504395\n",
            "Epoch 1, Batch 9391, Loss: 1.312995433807373\n",
            "Epoch 1, Batch 9401, Loss: 1.3147460222244263\n",
            "Epoch 1, Batch 9411, Loss: 1.3089910745620728\n",
            "Epoch 1, Batch 9421, Loss: 1.3085851669311523\n",
            "Epoch 1, Batch 9431, Loss: 1.3134145736694336\n",
            "Epoch 1, Batch 9441, Loss: 1.313897967338562\n",
            "Epoch 1, Batch 9451, Loss: 1.3120967149734497\n",
            "Epoch 1, Batch 9461, Loss: 1.3137364387512207\n",
            "Epoch 1, Batch 9471, Loss: 1.3174922466278076\n",
            "Epoch 1, Batch 9481, Loss: 1.3201216459274292\n",
            "Epoch 1, Batch 9491, Loss: 1.3057557344436646\n",
            "Epoch 1, Batch 9501, Loss: 1.3088821172714233\n",
            "Epoch 1, Batch 9511, Loss: 1.3075008392333984\n",
            "Epoch 1, Batch 9521, Loss: 1.3077105283737183\n",
            "Epoch 1, Batch 9531, Loss: 1.3194303512573242\n",
            "Epoch 1, Batch 9541, Loss: 1.3238334655761719\n",
            "Epoch 1, Batch 9551, Loss: 1.325106143951416\n",
            "Epoch 1, Batch 9561, Loss: 1.329819679260254\n",
            "Epoch 1, Batch 9571, Loss: 1.312430739402771\n",
            "Epoch 1, Batch 9581, Loss: 1.3239920139312744\n",
            "Epoch 1, Batch 9591, Loss: 1.3273762464523315\n",
            "Epoch 1, Batch 9601, Loss: 1.3156031370162964\n",
            "Epoch 1, Batch 9611, Loss: 1.3113822937011719\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_model(lora_model, train_loader, optimizer, criterion, device, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6zkbruInME1",
        "outputId": "cb59677b-8aa4-4c2b-f174-009669267732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\naverage loss:\\n\\n'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Already saved loss for the training. the checkpoints are also saved\n",
        "\n",
        "#loss history for under 240\n",
        "loss_history =[1.2644726806950528,\n",
        " 1.242887128370729,\n",
        " 1.2066624065750828,\n",
        " 1.2070949814920082,\n",
        " 1.206244443510456,\n",
        " 1.2060478101701662,\n",
        " 1.205697778779031,\n",
        " 1.2053182708883476,\n",
        " 1.2058163822979422,\n",
        " 1.2062993284159562]\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "average loss for multipication approach:\n",
        " 2, Batch 11801, Loss: 1.0300836563110352\n",
        "Epoch 2, Batch 11811, Loss: 1.082323431968689\n",
        "Epoch 2, Batch 11821, Loss: 1.0216240882873535\n",
        "Epoch 2, Batch 11831, Loss: 1.0629262924194336\n",
        "Epoch 2, Batch 11841, Loss: 1.0521647930145264\n",
        "Epoch 2, Batch 11851, Loss: 1.0114256143569946\n",
        "Epoch 2, Batch 11861, Loss: 1.0096522569656372\n",
        "Epoch 2, Batch 11871, Loss: 1.0902949571609497\n",
        "Epoch 2, Batch 11881, Loss: 1.03969144821167\n",
        "Epoch 2, Batch 11891, Loss: 1.0338724851608276\n",
        "Epoch 2, Batch 11901, Loss: 1.0221905708312988\n",
        "Epoch 2, Batch 11911, Loss: 1.021114468574524\n",
        "Epoch 2, Batch 11921, Loss: 1.0229949951171875\n",
        "Epoch 2, Batch 11931, Loss: 1.037742018699646\n",
        "Epoch 2, Batch 11941, Loss: 1.0248881578445435\n",
        "Epoch 2, Batch 11951, Loss: 1.0436757802963257\n",
        "Epoch 2, Batch 11961, Loss: 1.025509238243103\n",
        "Epoch 2, Batch 11971, Loss: 1.0162767171859741\n",
        "Epoch 2, Batch 11981, Loss: 1.027431607246399\n",
        "Epoch 2, Batch 11991, Loss: 1.017958641052246\n",
        "End of Epoch 2, Average Loss: 2.0710250906064718\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Epoch 2, Batch 11921, Loss: 0.6252635717391968\n",
        "Epoch 2, Batch 11931, Loss: 0.6343806982040405\n",
        "Epoch 2, Batch 11941, Loss: 0.6306359171867371\n",
        "Epoch 2, Batch 11951, Loss: 0.6510419845581055\n",
        "Epoch 2, Batch 11961, Loss: 0.665947437286377\n",
        "Epoch 2, Batch 11971, Loss: 0.6621376276016235\n",
        "Epoch 2, Batch 11981, Loss: 0.6571395397186279\n",
        "Epoch 2, Batch 11991, Loss: 0.6304044723510742\n",
        "End of Epoch 2, Average Loss: 1.3146582167791954\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "MhuwIp6hYuvn",
        "outputId": "6f656e33-8f6f-4c14-d066-a45f8d845a1e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_history):\n",
        "    plt.plot(loss_history, label='Training Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'loss_history' is obtained from 'train_model' function after the model training is complete.\n",
        "plot_loss(loss_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WqOJe_5nME2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        dataloader (DataLoader): The DataLoader for the dataset to evaluate.\n",
        "        criterion (torch.nn.Module): The loss function used for evaluation.\n",
        "        device (torch.device): The device tensors are sent to (GPU or CPU).\n",
        "\n",
        "    Returns:\n",
        "        list, float, float: The loss history for each batch, and the average loss and accuracy over the evaluation dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "    loss_history_val = []  # List to store loss of each batch\n",
        "\n",
        "    print(\"Starting evaluation...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1))\n",
        "            loss_history_val.append(loss.item())  # Append the current loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert outputs to probabilities and then to predicted labels\n",
        "            probs = torch.softmax(outputs, dim=-1)\n",
        "            predictions = torch.argmax(probs, dim=-1)\n",
        "            correct = (predictions == labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_examples += labels.nelement()\n",
        "\n",
        "            if i % 10 == 0:  # Print every 10 batches\n",
        "                print(f'Batch {i+1}: Loss: {loss.item():.4f}, Accuracy: {correct/labels.nelement():.4f}')\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_examples\n",
        "    print(f'Finished evaluation - Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "    return loss_history_val, average_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU_39h_FnME3",
        "outputId": "045d7fc9-73c7-470c-8f96-b04a3cdadf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation...\n",
            "Batch 1: Loss: 1.2339, Accuracy: 0.6535\n",
            "Batch 11: Loss: 1.1810, Accuracy: 0.8758\n",
            "Batch 21: Loss: 1.1764, Accuracy: 0.7380\n",
            "Batch 31: Loss: 1.2086, Accuracy: 0.8458\n",
            "Batch 41: Loss: 1.1979, Accuracy: 0.6595\n",
            "Batch 51: Loss: 1.1945, Accuracy: 0.7558\n",
            "Batch 61: Loss: 1.1849, Accuracy: 0.8505\n",
            "Batch 71: Loss: 1.1878, Accuracy: 0.7315\n",
            "Batch 81: Loss: 1.2126, Accuracy: 0.6653\n",
            "Batch 91: Loss: 1.1950, Accuracy: 0.6388\n",
            "Batch 101: Loss: 1.2002, Accuracy: 0.7007\n",
            "Batch 111: Loss: 1.2012, Accuracy: 0.7508\n",
            "Batch 121: Loss: 1.1775, Accuracy: 0.6577\n",
            "Batch 131: Loss: 1.1992, Accuracy: 0.5290\n",
            "Batch 141: Loss: 1.1845, Accuracy: 0.7705\n",
            "Batch 151: Loss: 1.1726, Accuracy: 0.7192\n",
            "Batch 161: Loss: 1.2319, Accuracy: 0.6420\n",
            "Batch 171: Loss: 1.2267, Accuracy: 0.7232\n",
            "Batch 181: Loss: 1.1901, Accuracy: 0.7907\n",
            "Batch 191: Loss: 1.2086, Accuracy: 0.7238\n",
            "Batch 201: Loss: 1.2378, Accuracy: 0.6957\n",
            "Batch 211: Loss: 1.1899, Accuracy: 0.7308\n",
            "Batch 221: Loss: 1.2028, Accuracy: 0.6530\n",
            "Batch 231: Loss: 1.2251, Accuracy: 0.6870\n",
            "Batch 241: Loss: 1.1897, Accuracy: 0.8177\n",
            "Batch 251: Loss: 1.2114, Accuracy: 0.6370\n",
            "Batch 261: Loss: 1.1948, Accuracy: 0.7980\n",
            "Batch 271: Loss: 1.2034, Accuracy: 0.7642\n",
            "Batch 281: Loss: 1.2022, Accuracy: 0.7113\n",
            "Batch 291: Loss: 1.1816, Accuracy: 0.7113\n",
            "Batch 301: Loss: 1.1988, Accuracy: 0.6525\n",
            "Batch 311: Loss: 1.2152, Accuracy: 0.6620\n",
            "Batch 321: Loss: 1.2021, Accuracy: 0.7040\n",
            "Batch 331: Loss: 1.2095, Accuracy: 0.7220\n",
            "Batch 341: Loss: 1.1750, Accuracy: 0.7880\n",
            "Batch 351: Loss: 1.1883, Accuracy: 0.6985\n",
            "Batch 361: Loss: 1.2168, Accuracy: 0.6850\n",
            "Batch 371: Loss: 1.1856, Accuracy: 0.6575\n",
            "Batch 381: Loss: 1.2161, Accuracy: 0.7762\n",
            "Batch 391: Loss: 1.2303, Accuracy: 0.5877\n",
            "Batch 401: Loss: 1.1999, Accuracy: 0.5960\n",
            "Batch 411: Loss: 1.1745, Accuracy: 0.7505\n",
            "Batch 421: Loss: 1.1960, Accuracy: 0.6705\n",
            "Batch 431: Loss: 1.1978, Accuracy: 0.7165\n",
            "Batch 441: Loss: 1.2082, Accuracy: 0.8283\n",
            "Batch 451: Loss: 1.1961, Accuracy: 0.6332\n",
            "Batch 461: Loss: 1.2505, Accuracy: 0.5132\n",
            "Batch 471: Loss: 1.1937, Accuracy: 0.8442\n",
            "Batch 481: Loss: 1.2195, Accuracy: 0.6663\n",
            "Batch 491: Loss: 1.1923, Accuracy: 0.5563\n",
            "Batch 501: Loss: 1.2360, Accuracy: 0.6332\n",
            "Batch 511: Loss: 1.2030, Accuracy: 0.6110\n",
            "Batch 521: Loss: 1.1990, Accuracy: 0.7415\n",
            "Batch 531: Loss: 1.2014, Accuracy: 0.7003\n",
            "Batch 541: Loss: 1.1868, Accuracy: 0.6795\n",
            "Batch 551: Loss: 1.2325, Accuracy: 0.6462\n",
            "Batch 561: Loss: 1.1924, Accuracy: 0.7887\n",
            "Batch 571: Loss: 1.1994, Accuracy: 0.7212\n",
            "Batch 581: Loss: 1.1810, Accuracy: 0.7933\n",
            "Batch 591: Loss: 1.1901, Accuracy: 0.6090\n",
            "Batch 601: Loss: 1.1785, Accuracy: 0.7167\n",
            "Batch 611: Loss: 1.2197, Accuracy: 0.8043\n",
            "Batch 621: Loss: 1.1947, Accuracy: 0.7053\n",
            "Batch 631: Loss: 1.2232, Accuracy: 0.6018\n",
            "Batch 641: Loss: 1.1810, Accuracy: 0.7823\n",
            "Batch 651: Loss: 1.2094, Accuracy: 0.7795\n",
            "Batch 661: Loss: 1.2060, Accuracy: 0.7445\n",
            "Batch 671: Loss: 1.1942, Accuracy: 0.6647\n",
            "Batch 681: Loss: 1.1780, Accuracy: 0.7572\n",
            "Batch 691: Loss: 1.2005, Accuracy: 0.7160\n",
            "Batch 701: Loss: 1.1733, Accuracy: 0.7475\n",
            "Batch 711: Loss: 1.2084, Accuracy: 0.7670\n",
            "Batch 721: Loss: 1.2569, Accuracy: 0.6705\n",
            "Batch 731: Loss: 1.2049, Accuracy: 0.7515\n",
            "Batch 741: Loss: 1.1964, Accuracy: 0.8215\n",
            "Batch 751: Loss: 1.1808, Accuracy: 0.8175\n",
            "Batch 761: Loss: 1.2054, Accuracy: 0.6575\n",
            "Batch 771: Loss: 1.1805, Accuracy: 0.7740\n",
            "Batch 781: Loss: 1.2158, Accuracy: 0.7768\n",
            "Batch 791: Loss: 1.1894, Accuracy: 0.7085\n",
            "Batch 801: Loss: 1.1892, Accuracy: 0.7642\n",
            "Batch 811: Loss: 1.1769, Accuracy: 0.7755\n",
            "Batch 821: Loss: 1.1918, Accuracy: 0.7127\n",
            "Batch 831: Loss: 1.2157, Accuracy: 0.5845\n",
            "Batch 841: Loss: 1.1912, Accuracy: 0.6860\n",
            "Batch 851: Loss: 1.1688, Accuracy: 0.8283\n",
            "Batch 861: Loss: 1.2248, Accuracy: 0.7712\n",
            "Batch 871: Loss: 1.2089, Accuracy: 0.7943\n",
            "Batch 881: Loss: 1.1659, Accuracy: 0.8280\n",
            "Batch 891: Loss: 1.2191, Accuracy: 0.7762\n",
            "Batch 901: Loss: 1.2003, Accuracy: 0.7678\n",
            "Batch 911: Loss: 1.1871, Accuracy: 0.7410\n",
            "Batch 921: Loss: 1.1974, Accuracy: 0.6660\n",
            "Batch 931: Loss: 1.1754, Accuracy: 0.7522\n",
            "Batch 941: Loss: 1.1969, Accuracy: 0.6673\n",
            "Batch 951: Loss: 1.2227, Accuracy: 0.6725\n",
            "Batch 961: Loss: 1.1895, Accuracy: 0.7117\n",
            "Batch 971: Loss: 1.2129, Accuracy: 0.7147\n",
            "Batch 981: Loss: 1.1878, Accuracy: 0.7515\n",
            "Batch 991: Loss: 1.1713, Accuracy: 0.7500\n",
            "Batch 1001: Loss: 1.2240, Accuracy: 0.7033\n",
            "Batch 1011: Loss: 1.1956, Accuracy: 0.6853\n",
            "Batch 1021: Loss: 1.2119, Accuracy: 0.7883\n",
            "Batch 1031: Loss: 1.2321, Accuracy: 0.7095\n",
            "Batch 1041: Loss: 1.2257, Accuracy: 0.6430\n",
            "Batch 1051: Loss: 1.2022, Accuracy: 0.6745\n",
            "Batch 1061: Loss: 1.2313, Accuracy: 0.6342\n",
            "Batch 1071: Loss: 1.1959, Accuracy: 0.7000\n",
            "Batch 1081: Loss: 1.2095, Accuracy: 0.6258\n",
            "Batch 1091: Loss: 1.1948, Accuracy: 0.8828\n",
            "Batch 1101: Loss: 1.2009, Accuracy: 0.6330\n",
            "Batch 1111: Loss: 1.2283, Accuracy: 0.6867\n",
            "Batch 1121: Loss: 1.2310, Accuracy: 0.5305\n",
            "Batch 1131: Loss: 1.1716, Accuracy: 0.8430\n",
            "Batch 1141: Loss: 1.1965, Accuracy: 0.7378\n",
            "Batch 1151: Loss: 1.2110, Accuracy: 0.7163\n",
            "Batch 1161: Loss: 1.1976, Accuracy: 0.7063\n",
            "Batch 1171: Loss: 1.1953, Accuracy: 0.6310\n",
            "Batch 1181: Loss: 1.2003, Accuracy: 0.6680\n",
            "Batch 1191: Loss: 1.2344, Accuracy: 0.4562\n",
            "Batch 1201: Loss: 1.2130, Accuracy: 0.7003\n",
            "Batch 1211: Loss: 1.2066, Accuracy: 0.7192\n",
            "Batch 1221: Loss: 1.2077, Accuracy: 0.7260\n",
            "Batch 1231: Loss: 1.2007, Accuracy: 0.6472\n",
            "Batch 1241: Loss: 1.1924, Accuracy: 0.6178\n",
            "Batch 1251: Loss: 1.2066, Accuracy: 0.7202\n",
            "Batch 1261: Loss: 1.1883, Accuracy: 0.7530\n",
            "Batch 1271: Loss: 1.2046, Accuracy: 0.8100\n",
            "Batch 1281: Loss: 1.1955, Accuracy: 0.6955\n",
            "Batch 1291: Loss: 1.1967, Accuracy: 0.6715\n",
            "Batch 1301: Loss: 1.1990, Accuracy: 0.7800\n",
            "Batch 1311: Loss: 1.2103, Accuracy: 0.8263\n",
            "Finished evaluation - Average Loss: 1.2028, Accuracy: 0.7093\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assume model, validation_dataloader, criterion, and device are already defined\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m average_loss_val, accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, criterion, device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation results - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# Assume model, validation_dataloader, criterion, and device are already defined\n",
        "average_loss_val, accuracy = evaluate_model(model, val_loader, criterion, device)\n",
        "print(f\"Validation results - Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_rPyJs0nME3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history_val, label='Loss per Batch')\n",
        "plt.title('Evaluation Loss History')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ZBHUOSnME3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (MainProject)",
      "language": "python",
      "name": "mainproject"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}