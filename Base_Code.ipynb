{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/SASA-Calculation-For-LLMs/blob/main/Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e39a80b-6dfc-41bd-926c-12f22da749cf",
      "metadata": {
        "id": "7e39a80b-6dfc-41bd-926c-12f22da749cf"
      },
      "source": [
        "# Code Agenda:\n",
        "- Provide global sequence and SASA values as separate inputs.\n",
        "- Use the local sequence as the label.\n",
        "- No new tokens are added; it's a simple prediction task.\n",
        "- Build up from this.\n",
        "\n",
        "# Code Outline:\n",
        "1. Import the dataset.\n",
        "2. Import tokenizer/base model.\n",
        "3. Build dataset class.\n",
        "4. Build the main model architecture.\n",
        "5. Create the DataLoader.\n",
        "6. Split data into training and testing sets.\n",
        "7. Train the model.\n",
        "8. Test the model.\n",
        "9. Make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a18cd3b-fb23-4826-8be8-23d47c9d28c1",
      "metadata": {
        "id": "2a18cd3b-fb23-4826-8be8-23d47c9d28c1"
      },
      "outputs": [],
      "source": [
        "#import the libraries:\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727e6a23-ce86-4a99-a80c-b95bbace0296",
      "metadata": {
        "id": "727e6a23-ce86-4a99-a80c-b95bbace0296"
      },
      "outputs": [],
      "source": [
        "# Clear the CUDA memory cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#collect garbage to free up memory from unused objects\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75320e02-b7f6-4f8c-ad8e-3c7a48b48a7b",
      "metadata": {
        "id": "75320e02-b7f6-4f8c-ad8e-3c7a48b48a7b"
      },
      "source": [
        "# 1.Import the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6d8953-4c07-4c3f-9a30-91062a2fb0ca",
      "metadata": {
        "id": "af6d8953-4c07-4c3f-9a30-91062a2fb0ca"
      },
      "outputs": [],
      "source": [
        "#THIS SHOULD BE MODIFIED:  this is the path for the main dataset\n",
        "pairs_df = pd.read_csv('/home/k_ensafitakaldani001_umb_edu/Project1/merged1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2197824-f385-4676-bde0-0444bc5fd637",
      "metadata": {
        "id": "b2197824-f385-4676-bde0-0444bc5fd637"
      },
      "outputs": [],
      "source": [
        "pairs_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394d8518-6c91-433c-90b5-4788ed0065f7",
      "metadata": {
        "id": "394d8518-6c91-433c-90b5-4788ed0065f7"
      },
      "outputs": [],
      "source": [
        "#Just some cleaning up needed for dataset: No need to get to deep in this part\n",
        "\n",
        "#pairs_df.head(10)\n",
        "len(pairs_df)\n",
        "\n",
        "# Count the number of NaN values in the 'SASA_A' column\n",
        "nan_count = pairs_df['SASA_A'].isna().sum()\n",
        "\n",
        "print(nan_count)\n",
        "\n",
        "\n",
        "# Drop rows where the 'sasa_A' column has NaN values\n",
        "df_cleaned = pairs_df.dropna(subset=['SASA_A'])\n",
        "\n",
        "len(df_cleaned)\n",
        "\n",
        "pairs_df = df_cleaned\n",
        "\n",
        "len(pairs_df)\n",
        "\n",
        "df = pairs_df\n",
        "filtered_df = df[(df['Sequence_A'].str.len() >= 50) & (df['Sequence_A'].str.len() < 200)]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df\n",
        "\n",
        "\n",
        "# Count the number of NaN values in the 'SASA_A' column\n",
        "nan_count = pairs_df['SASA_A'].isna().sum()\n",
        "\n",
        "print(nan_count)\n",
        "\n",
        "df = pairs_df\n",
        "filtered_df = df[(df['Sequence_A'].str.len() >= 50) & (df['Sequence_A'].str.len() < 200)]\n",
        "print(len(filtered_df))\n",
        "pairs_df = filtered_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e8c201-5cbc-4577-a12d-b65b1701041d",
      "metadata": {
        "id": "65e8c201-5cbc-4577-a12d-b65b1701041d"
      },
      "source": [
        "# 2.Import tokenizer/base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db1d2f2-38c5-4629-9696-75bed9202998",
      "metadata": {
        "id": "0db1d2f2-38c5-4629-9696-75bed9202998"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import BertModel, BertConfig, AutoTokenizer\n",
        "\n",
        "\n",
        "'''\n",
        "Initialize the ProtBERT tokenizer and model -> mainly use these for pretraining\n",
        "used for a variety of downstream tasks (e.g., classification, tagging).\n",
        "Unlike AutoModelForMaskedLM, it is not specifically tied to masked language modeling\n",
        "'''\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd')\n",
        "\n",
        "# Define special tokens for entities -> our new tokens for seperation and sasa classification\n",
        "special_tokens = ['[ENTITY1]', '[ENTITY2]', '-', 'BR', 'PE', 'EX']\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Check if the special tokens were added successfully\n",
        "print(f\"Token '[ENTITY1]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY1]')}\")\n",
        "print(f\"Token '[ENTITY2]' has ID: {tokenizer.convert_tokens_to_ids('[ENTITY2]')}\")\n",
        "print(f\"Token '-' has ID: {tokenizer.convert_tokens_to_ids('-')}\")\n",
        "print(f\"Token 'BR' has ID: {tokenizer.convert_tokens_to_ids('BR')}\")\n",
        "print(f\"Token 'PE' has ID: {tokenizer.convert_tokens_to_ids('PE')}\")\n",
        "print(f\"Token 'EX' has ID: {tokenizer.convert_tokens_to_ids('EX')}\")\n",
        "\n",
        "# Resize the model's embedding size to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print('Token embeddings resized to accommodate new tokens.')\n",
        "\n",
        "# Helper function to convert numerical token IDs back to their textual representation\n",
        "def ids_to_text(ids):\n",
        "    return ' '.join(tokenizer.convert_ids_to_tokens(ids))\n",
        "\n",
        "# Check the updated size of the tokenizer's vocabulary\n",
        "print(f\"Updated vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "# Check if the new tokens are in the tokenizer's vocabulary\n",
        "if all(token in tokenizer.get_vocab() for token in special_tokens):\n",
        "    print(\"All special tokens are in the tokenizer's vocabulary.\")\n",
        "else:\n",
        "    print(\"Some special tokens are NOT in the tokenizer's vocabulary.\")\n",
        "\n",
        "\n",
        "#some checks:\n",
        "vocab= tokenizer.get_vocab()\n",
        "print(len(vocab))\n",
        "\n",
        "# Get the number of amino acids\n",
        "num_amino_acids = len(tokenizer.get_vocab())\n",
        "print(num_amino_acids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c61ac47-78d0-4c90-93c3-ea367eda1410",
      "metadata": {
        "id": "4c61ac47-78d0-4c90-93c3-ea367eda1410"
      },
      "source": [
        "#  3.Build dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c73f08a-6ad6-484f-9a2f-76ed99cbbdd3",
      "metadata": {
        "id": "8c73f08a-6ad6-484f-9a2f-76ed99cbbdd3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_SASA_sequence(self, row):\n",
        "        SASA_A = row['SASA_A']\n",
        "        SASA_B = row['SASA_B']\n",
        "\n",
        "        try:\n",
        "            if isinstance(SASA_A, str):\n",
        "                SASA_A = SASA_A.split(\", \")\n",
        "            elif not isinstance(SASA_A, list):\n",
        "                SASA_A = [str(SASA_A)]\n",
        "\n",
        "            if isinstance(SASA_B, str):\n",
        "                SASA_B = SASA_B.split(\", \")\n",
        "            elif not isinstance(SASA_B, list):\n",
        "                SASA_B = [str(SASA_B)]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing SASA sequences: {e}\")\n",
        "            print(f\"SASA_A: {SASA_A}, SASA_B: {SASA_B}\")\n",
        "\n",
        "        SASA_sequence = f\"[ENTITY1] {' '.join(SASA_A)} [SEP] [ENTITY2] {' '.join(SASA_B)}\"\n",
        "        return SASA_sequence\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx]\n",
        "        Global_Sequence_A = row['Sequence_A']\n",
        "        Global_Sequence_B = row['Sequence_B']\n",
        "        Local_Sequence_A = row['masked_sequence_A']\n",
        "        Local_Sequence_B = row['masked_sequence_B']\n",
        "\n",
        "        # Enhanced sequences with new tokens\n",
        "        Global_sequence = f\"[ENTITY1] {Global_Sequence_A} [SEP] [ENTITY2] {Global_Sequence_B}\"\n",
        "        Local_sequence = f\"[ENTITY1] {Local_Sequence_A} [SEP] [ENTITY2] {Local_Sequence_B}\"\n",
        "        SASA_sequence = self.get_SASA_sequence(row)\n",
        "\n",
        "        # Tokenize input, label, and SASA sequences\n",
        "        global_inputs = self.tokenizer(Global_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        labels = self.tokenizer(Local_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "        SASA_inputs = self.tokenizer(SASA_sequence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_len)\n",
        "\n",
        "        # Ensure all outputs are properly returned\n",
        "        return {\n",
        "            'global_input_ids': global_inputs['input_ids'].squeeze(0),\n",
        "            'global_attention_mask': global_inputs['attention_mask'].squeeze(0),\n",
        "            'SASA_input_ids': SASA_inputs['input_ids'].squeeze(0),  # SASA tokenized and part of input\n",
        "            'SASA_attention_mask': SASA_inputs['attention_mask'].squeeze(0),  # Attention mask for SASA inputs\n",
        "            'labels': labels['input_ids'].squeeze(0)  # Local sequences as labels\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs = {k: torch.stack([d[k] for d in batch]) for k in batch[0]}\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd3abf6-eccc-4891-be40-660dda75ff98",
      "metadata": {
        "id": "3dd3abf6-eccc-4891-be40-660dda75ff98"
      },
      "source": [
        "# 4. Build the main model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b0d260-6883-4d1c-8d1c-dc337fcb6f3f",
      "metadata": {
        "id": "a0b0d260-6883-4d1c-8d1c-dc337fcb6f3f"
      },
      "outputs": [],
      "source": [
        "# Gated Mechanism with Improvements\n",
        "\n",
        "class ProtBertSeq2Seq(nn.Module):\n",
        "    def __init__(self, model, num_amino_acids, seq_len, dropout_rate=0.1):\n",
        "        super(ProtBertSeq2Seq, self).__init__()\n",
        "        self.model = model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_amino_acids = num_amino_acids\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Improved Gating mechanism using MLP\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(model.config.hidden_size * 2, model.config.hidden_size),\n",
        "            nn.ReLU(),  # ReLU for non-linearity\n",
        "            nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
        "            nn.Sigmoid()  # Can experiment with other activations\n",
        "        )\n",
        "\n",
        "        # Dynamic learnable scalars for global and SASA outputs\n",
        "        self.alpha = nn.Parameter(torch.randn(1))  # Dynamically learnable alpha\n",
        "        self.beta = nn.Parameter(torch.randn(1))   # Dynamically learnable beta\n",
        "\n",
        "        # Incorporate multi-head attention for combining global and SASA outputs\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=model.config.hidden_size, num_heads=4)\n",
        "\n",
        "        # Adding layer normalization to stabilize training\n",
        "        self.layer_norm = nn.LayerNorm(model.config.hidden_size)\n",
        "\n",
        "        # Option for residual connections\n",
        "        self.use_residual = True  # Option to use residual connections\n",
        "\n",
        "        # Classifier for prediction\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(model.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_amino_acids)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask):\n",
        "        # Process global sequence inputs\n",
        "        global_outputs = self.model(input_ids=global_input_ids, attention_mask=global_attention_mask).last_hidden_state\n",
        "        global_outputs = self.dropout(global_outputs)\n",
        "\n",
        "        # Process SASA sequence inputs\n",
        "        SASA_outputs = self.model(input_ids=SASA_input_ids, attention_mask=SASA_attention_mask).last_hidden_state\n",
        "        SASA_outputs = self.dropout(SASA_outputs)\n",
        "\n",
        "        # Multi-head attention over global and SASA outputs\n",
        "        attention_output, _ = self.attention(global_outputs, SASA_outputs, SASA_outputs)\n",
        "\n",
        "        # Concatenate global and SASA outputs with attention output\n",
        "        combined_inputs = torch.cat((attention_output, SASA_outputs), dim=-1)\n",
        "\n",
        "        # Apply improved gating mechanism (MLP-based)\n",
        "        gate_output = self.gate_mlp(combined_inputs)\n",
        "        mixed_outputs = gate_output * (self.alpha * global_outputs) + (1 - gate_output) * (self.beta * SASA_outputs)\n",
        "\n",
        "        # Apply residual connections if enabled\n",
        "        if self.use_residual:\n",
        "            mixed_outputs += global_outputs + SASA_outputs  # Residual connections\n",
        "\n",
        "        # Apply layer normalization to stabilize the mixed outputs\n",
        "        mixed_outputs = self.layer_norm(mixed_outputs)\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(mixed_outputs)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2cc8ee0-f0d1-4e35-b82d-fd327457451f",
      "metadata": {
        "tags": [],
        "id": "a2cc8ee0-f0d1-4e35-b82d-fd327457451f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This section is still part of building the model's main architecture.\n",
        "We're just defining the training and evaluation functions.\n",
        "'''\n",
        "\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# Function to save checkpoint -> DONT FORGET TO MODIFY THE PATH IN YOUR SYSTEM\n",
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# Training function with updated autocast and GradScaler\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "# Training function with updated autocast and GradScaler\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs, accumulation_steps=2, checkpoint_path=\"checkpoint240.pth.tar\"):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    start_epoch = 0\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "    scaler = GradScaler()  # Updated mixed precision scaler\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss_history = checkpoint['loss_history']\n",
        "        val_loss_history = checkpoint.get('val_loss_history', [])\n",
        "        val_accuracy_history = checkpoint.get('val_accuracy_history', [])\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {checkpoint['epoch']})\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        total_loss = 0\n",
        "        model.train()  # Ensure model is in training mode\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Corrected autocast syntax\n",
        "            with autocast():\n",
        "                outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "                loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1)) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()  # Scaled backward pass\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:  # Perform optimizer step every few batches\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps  # Accumulate the loss\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}')\n",
        "\n",
        "            # Freeing memory after each batch\n",
        "            del global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask, labels, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        loss_history.append(average_loss)\n",
        "        print(f'End of Epoch {epoch + 1}, Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # --- Evaluate the model on the validation set after each epoch ---\n",
        "        avg_val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_model_with_metrics(model, val_loader, criterion, device)\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "        print(f'Epoch {epoch + 1} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}')\n",
        "\n",
        "        # Save checkpoint after every epoch\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss_history': loss_history,\n",
        "            'val_loss_history': val_loss_history,\n",
        "            'val_accuracy_history': val_accuracy_history\n",
        "        }, filename=checkpoint_path)\n",
        "\n",
        "    return loss_history, val_loss_history, val_accuracy_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_with_metrics(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during evaluation\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            global_input_ids = batch['global_input_ids'].to(device)\n",
        "            global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "            SASA_input_ids = batch['SASA_input_ids'].to(device)\n",
        "            SASA_attention_mask = batch['SASA_attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Use autocast for mixed precision evaluation\n",
        "            with autocast():\n",
        "                outputs = model(global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask)\n",
        "                loss = criterion(outputs.view(-1, model.num_amino_acids), labels.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and labels\n",
        "            predictions = torch.argmax(outputs, dim=-1)\n",
        "            all_predictions.extend(predictions.view(-1).cpu().numpy())  # Flatten and convert to NumPy array\n",
        "            all_labels.extend(labels.view(-1).cpu().numpy())  # Flatten and convert to NumPy array\n",
        "\n",
        "            # Free memory after each batch\n",
        "            del global_input_ids, global_attention_mask, SASA_input_ids, SASA_attention_mask, labels, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Convert to NumPy arrays for metric calculation\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate Precision, Recall, and F1-Score (macro-average)\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = (all_predictions == all_labels).sum() / len(all_labels)\n",
        "\n",
        "    print(f'Evaluation completed, Average Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
        "\n",
        "    return average_loss, accuracy, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1d2477-38a8-4cf0-b466-fe35718da2eb",
      "metadata": {
        "id": "7b1d2477-38a8-4cf0-b466-fe35718da2eb"
      },
      "source": [
        "# 5. Create the DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24f784e-f20a-4654-8cad-00577e9fcd84",
      "metadata": {
        "id": "d24f784e-f20a-4654-8cad-00577e9fcd84"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split #split the train and test\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "train_df, val_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Manually reduce the validation DataFrame size by taking 20% of the original validation set\n",
        "val_df = val_df.sample(frac=0.5, random_state=42)  # Keep 20% of the original validation split\n",
        "\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "\n",
        "# Setup DataLoaders for training and validation\n",
        "train_dataset = SampleDataset(train_df, tokenizer,500)\n",
        "val_dataset = SampleDataset(val_df, tokenizer, 500)\n",
        "\n",
        "# Using pin_memory for faster host to device transfer\n",
        "# Increasing num_workers to use multiple CPU cores for data loading\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn,  num_workers=2, pin_memory=True)\n",
        "\n",
        "# Log the setup\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}, Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}, Each batch has {val_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40453568-2239-4b7e-98ca-8fd941e8d56d",
      "metadata": {
        "id": "40453568-2239-4b7e-98ca-8fd941e8d56d"
      },
      "source": [
        "# 7. Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492001a8-4bca-40b0-884d-a370f39a51b4",
      "metadata": {
        "id": "492001a8-4bca-40b0-884d-a370f39a51b4"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam, lr_scheduler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "SEQ_LEN = 500\n",
        "LR_STEP_SIZE = 2\n",
        "LR_GAMMA = 0.5\n",
        "ACCUMULATION_STEPS = 2\n",
        "NUM_EPOCHS = 10\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = tokenizer\n",
        "\n",
        "# Load the base BertModel or similar from transformers suited for your needs\n",
        "base_model = model  # Replace with your base model\n",
        "\n",
        "# Get the number of amino acids (adjust based on whether you are using a classification task)\n",
        "num_amino_acids = len(vocab)  # Adjust this if your task isn't directly classification\n",
        "\n",
        "# Initialize your custom model with the base model\n",
        "my_model = ProtBertSeq2Seq(model=base_model, num_amino_acids=num_amino_acids, seq_len=500)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#lora_model.to(device)\n",
        "\n",
        "my_model.to(device)\n",
        "\n",
        "# Optimizer and scheduler setup\n",
        "optimizer = Adam(my_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682292ce-7979-41d8-a3ca-f43babf82e63",
      "metadata": {
        "tags": [],
        "id": "682292ce-7979-41d8-a3ca-f43babf82e63"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Call the training function, which will also evaluate after each epoch\n",
        "train_loss_history, val_loss_history, val_accuracy_history = train_model(\n",
        "    my_model,  # Using the LoRA model\n",
        "    train_loader,  # Training data loader\n",
        "    val_loader,  # Validation data loader\n",
        "    optimizer,  # Optimizer for LoRA model\n",
        "    criterion,  # Loss function\n",
        "    device,  # Device (CPU or GPU)\n",
        "    num_epochs  # Number of epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc7c9da-4489-4deb-9693-16d61ee32446",
      "metadata": {
        "id": "8cc7c9da-4489-4deb-9693-16d61ee32446"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc448a48-2077-4948-a8f3-e8f6ff2a7228",
      "metadata": {
        "id": "dc448a48-2077-4948-a8f3-e8f6ff2a7228"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MainLLM",
      "language": "python",
      "name": "mainllm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}